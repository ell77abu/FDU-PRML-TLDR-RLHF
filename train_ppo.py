import torch
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from datasets import load_dataset

# --- 1. é…ç½® ---
sft_model_path = "./models/sft-tldr/final_checkpoint"
rm_model_path = "./models/rm-tldr/final_rm"  # ä½ æœ€æ–°è®­ç»ƒå¥½çš„ RM

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
device = 'cuda' if torch.cuda.is_available() else 'cpu'

config = PPOConfig(
    model_name="qwen-1.8b-ppo",
    learning_rate=1.41e-5,
    batch_size=64,           # é‡‡æ ·æ€»æ•°
    mini_batch_size=1,       # ã€æ˜¾å­˜ä¼˜åŒ–ã€‘4090 å»ºè®®è®¾ä¸º 1ï¼Œé˜²æ­¢ Value Head è®¡ç®—æ—¶ OOM
    gradient_accumulation_steps=16,
    optimize_cuda_cache=True,
    target_kl=0.1,           # é™åˆ¶ç­–ç•¥åç¦» SFT å¤ªè¿œ
    init_kl_coeff=0.03,      # ã€é’ˆå¯¹ 0.43 åˆ†å·®ä¼˜åŒ–ã€‘åˆå§‹ KL è®¾å°ä¸€ç‚¹ï¼Œç»™å¾®å¼±å¥–åŠ±ç•™å‡ºç©ºé—´
    reward_whitening=True,   # ã€æ ¸å¿ƒã€‘å¼€å¯å¥–åŠ±å½’ä¸€åŒ–ï¼ˆç™½åŒ–ï¼‰ï¼Œå°† Batch å†…å¥–åŠ±è½¬ä¸ºå‡å€¼0ï¼Œæ–¹å·®1
)

# --- 2. åŠ è½½æ¨¡å‹ (é’ˆå¯¹ 24GB æ˜¾å­˜ä¼˜åŒ–) ---
tokenizer = AutoTokenizer.from_pretrained(sft_model_path)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" # PPO ç”Ÿæˆå¿…é¡»å·¦å¡«å……

# ç­–ç•¥æ¨¡å‹ (Policy)
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    sft_model_path,
    torch_dtype=torch.bfloat16,
    device_map={"": device}
)

# å‚è€ƒæ¨¡å‹ (Reference) - ç”¨äºè®¡ç®— KL æ•£åº¦
ref_model = create_reference_model(model)

# å¥–åŠ±æ¨¡å‹ (Reward Model)
reward_model = AutoModelForSequenceClassification.from_pretrained(
    rm_model_path,
    torch_dtype=torch.bfloat16,
    device_map={"": device}
).eval()

# --- 3. æ•°æ®åŠ è½½ ---
# ä½¿ç”¨ TL;DR ä»»åŠ¡çš„ Prompt éƒ¨åˆ†
dataset = load_dataset("CarperAI/openai_summarize_tldr", split="train")

def tokenize_fn(sample):
    # æ„é€ ä¸ RM è®­ç»ƒä¸€è‡´çš„ Prompt
    # åŸå§‹æ•°æ®é›†ä¸­ prompt å­—æ®µé€šå¸¸å·²åŒ…å« "Post: ... TL;DR:"
    sample["input_ids"] = tokenizer.encode(sample["prompt"], truncation=True, max_length=512)
    sample["query"] = sample["prompt"]
    return sample

dataset = dataset.shuffle(seed=42).map(tokenize_fn, batched=False)
dataset.set_format(type="torch")

def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}

# --- 4. åˆå§‹åŒ– PPO Trainer ---
ppo_trainer = PPOTrainer(
    config=config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
)

# --- 5. è®­ç»ƒå¾ªç¯ ---
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "max_new_tokens": 80, 
}

print("ğŸš€ PPO è®­ç»ƒå¯åŠ¨...")

for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # Step A: ç”Ÿæˆå“åº”
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

    # Step B: è®¡ç®—å¥–åŠ± (å¥–åŠ±å½’ä¸€åŒ–ä¸ç¼©æ”¾)
    tokenizer.padding_side = "right" # RM è¯„ä¼°å»ºè®®å³å¡«å……
    texts = [q + r + tokenizer.eos_token for q, r in zip(batch["query"], batch["response"])]
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    
    with torch.no_grad():
        # è·å–åŸå§‹ Logits åˆ†æ•°
        raw_rewards = reward_model(**inputs).logits.squeeze(-1)
        
        # ã€é‡è¦ï¼šå¥–åŠ±ç¼©æ”¾ã€‘
        # æ—¢ç„¶ä½ çš„å¹³å‡åˆ†å·®åªæœ‰ 0.43ï¼Œä¸ºäº†è®© PPO æ„Ÿè§‰åˆ°æ˜æ˜¾çš„å¥–æƒ©å·®å¼‚ï¼Œ
        # æˆ‘ä»¬åœ¨è¿™é‡Œä¹˜ä»¥ä¸€ä¸ªç¼©æ”¾ç³»æ•°ï¼ˆ2.0~3.0ï¼‰ï¼Œæ”¾å¤§ä¿¡å·å¼ºåº¦ã€‚
        rewards = [r * 2.5 for r in raw_rewards] 

    tokenizer.padding_side = "left" # æ¢å¤å·¦å¡«å……å‡†å¤‡ä¸‹ä¸€è½®

    # Step C: PPO ä¼˜åŒ–æ­¥
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    
    # æ‰“å°ç›‘æ§
    if epoch % 10 == 0:
        ppo_trainer.log_stats(stats, batch, rewards)
        print(f"Step {epoch} | Reward: {torch.mean(raw_rewards).item():.4f} | KL: {stats['objective/kl']:.4f}")

# --- 6. ä¿å­˜æ¨¡å‹ ---
model.save_pretrained("./qwen-1.8b-ppo-final")
tokenizer.save_pretrained("./qwen-1.8b-ppo-final")