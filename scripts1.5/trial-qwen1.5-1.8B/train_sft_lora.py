import os
import time
import torch
import wandb

# =========================
# æ”¹è¿›ç‰ˆ LoRA SFT è®­ç»ƒé…ç½®
# ä¸»è¦æ”¹è¿›ï¼š
# - å¢žåŠ æ•°æ®é‡åˆ°50000ä¸ªæ ·æœ¬
# - æé«˜LoRA rankåˆ°16ï¼Œæ‰©å±•åˆ°MLPå±‚
# - ä¼˜åŒ–å­¦ä¹ çŽ‡å’Œæ‰¹æ¬¡é…ç½®
# - æ·»åŠ æ•°æ®è´¨é‡è¿‡æ»¤
# - å¢žåŠ è®­ç»ƒè½®æ•°åˆ°3
# =========================
# 1. çŽ¯å¢ƒä¸ŽåŸºæœ¬é…ç½®
# =========================
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
device = 'cuda' if torch.cuda.is_available() else 'cpu'

model_id = './models/Qwen1.5-1.8B'
output_dir = "./sft-tldr-lora-improved"

run = wandb.init(
    project="prml-sft-lora-improved",
    name=f"qwen1.5-1.8b-lora-improved-{int(time.time())}",
    config={
        "train_sample": 50000,      # ä»Ž10000å¢žåŠ åˆ°50000
        "eval_sample": 2000,        # ç›¸åº”å¢žåŠ éªŒè¯é›†
        "learning_rate": 5e-5,      # ä»Ž1e-4é™ä½Žåˆ°5e-5
        "num_train_epochs": 3,       # ä»Ž1å¢žåŠ åˆ°3
        "batch_size": 2,            # ä»Ž1å¢žåŠ åˆ°2
        "grad_accum": 4,            # ä»Ž8é™ä½Žåˆ°4
        "max_seq_length": 512,      # ä»Ž768é™ä½Žåˆ°512
        "lora_r": 16,               # ä»Ž8å¢žåŠ åˆ°16
    },
)

# =========================
# 2. Tokenizer
# =========================
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=False,
    trust_remote_code=True,
    local_files_only=True,
)

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# =========================
# 3. æ•°æ®é›†
# =========================
from datasets import load_dataset
from trl import DataCollatorForCompletionOnlyLM

dataset = load_dataset("CarperAI/openai_summarize_tldr")

# å¢žåŠ æ•°æ®é‡
train_sample = 50000
eval_sample = 2000
test_sample = 1000

# æ•°æ®é¢„å¤„ç†å’Œè¿‡æ»¤ - æå–POSTå†…å®¹ï¼Œæé«˜è®­ç»ƒæ•ˆæžœ
def preprocess_and_filter(example):
    """é¢„å¤„ç†æ•°æ®ï¼šæå–POSTå†…å®¹ï¼Œè¿‡æ»¤ä½Žè´¨é‡æ ·æœ¬"""

    # æå–POSTå†…å®¹
    if "POST:" in example['prompt'] and "TL;DR:" in example['prompt']:
        post_start = example['prompt'].find("POST:")
        tldr_start = example['prompt'].find("TL;DR:")
        if post_start != -1 and tldr_start != -1:
            post_content = example['prompt'][post_start + 5:tldr_start].strip()
            # åˆ›å»ºç®€æ´çš„è¾“å…¥æ ¼å¼
            example['processed_prompt'] = f"{post_content}\nTL;DR:"
        else:
            example['processed_prompt'] = example['prompt']
    else:
        example['processed_prompt'] = example['prompt']

    # æ•°æ®è´¨é‡è¿‡æ»¤
    prompt_len = len(example['processed_prompt'].split())
    label_len = len(example['label'].split())

    # ä¿ç•™é«˜è´¨é‡æ ·æœ¬
    keep = (prompt_len > 30 and prompt_len < 200 and      # POSTå†…å®¹åˆç†é•¿åº¦
            label_len > 5 and label_len < 50 and           # æ‘˜è¦åˆç†é•¿åº¦
            label_len / prompt_len < 0.5)                  # åŽ‹ç¼©æ¯”åˆç†

    return keep

print(f"åŽŸå§‹æ•°æ®é›†å¤§å°: {len(dataset['train'])}")
dataset = dataset.filter(preprocess_and_filter)
print(f"è¿‡æ»¤åŽæ•°æ®é›†å¤§å°: {len(dataset['train'])}")

dataset_small = {
    "train": dataset["train"].select(range(min(train_sample, len(dataset["train"])))),
    "valid": dataset["valid"].select(range(min(eval_sample, len(dataset["valid"])))),
    "test": dataset["test"].select(range(min(test_sample, len(dataset["test"])))),
}

def formatting_prompts_func(example):
    """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬ï¼šä½¿ç”¨å¤„ç†åŽçš„prompt + æ ‡ç­¾"""
    texts = []
    for i in range(len(example["processed_prompt"])):
        text = f"{example['processed_prompt'][i]} {example['label'][i]}{tokenizer.eos_token}"
        texts.append(text)
    return texts

response_template = "TL;DR:"
collator = DataCollatorForCompletionOnlyLM(
    response_template=response_template,
    tokenizer=tokenizer,
)

# =========================
# 4. åŠ è½½æ¨¡åž‹ + æ³¨å…¥ LoRA
# =========================
from transformers import AutoModelForCausalLM
from peft import LoraConfig, get_peft_model, TaskType

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    local_files_only=True,
)

# æ”¹è¿›çš„LoRAé…ç½® - æé«˜è¡¨è¾¾èƒ½åŠ›å’Œè®­ç»ƒæ•ˆæžœ
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=16,                    # ä»Ž8å¢žåŠ åˆ°16ï¼Œæé«˜è¡¨è¾¾èƒ½åŠ›
    lora_alpha=32,           # ä»Ž16å¢žåŠ åˆ°32ï¼Œä¼˜åŒ–ç¼©æ”¾æ¯”ä¾‹
    lora_dropout=0.05,
    bias="none",
    target_modules=[         # æ‰©å±•åˆ°attentionå’ŒMLPå±‚ï¼Œæé«˜é€‚åº”æ€§
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

model.to(device)

# =========================
# 5. è®­ç»ƒå‚æ•°
# =========================
from transformers import TrainingArguments
from trl import SFTTrainer

training_args = TrainingArguments(
    output_dir=f"{output_dir}-improved",  # æ–°çš„è¾“å‡ºç›®å½•
    per_device_train_batch_size=2,        # ä»Ž1å¢žåŠ åˆ°2
    gradient_accumulation_steps=4,        # ä»Ž8é™ä½Žåˆ°4
    gradient_checkpointing=True,          # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜

    learning_rate=5e-5,                   # ä»Ž1e-4é™ä½Žåˆ°5e-5
    num_train_epochs=3,                   # ä»Ž1å¢žåŠ åˆ°3
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",

    logging_steps=50,                     # å‡å°‘æ—¥å¿—é¢‘çŽ‡
    eval_strategy="steps",
    eval_steps=500,                       # å¢žåŠ è¯„ä¼°é—´éš”
    save_strategy="steps",
    save_steps=1000,                      # å¢žåŠ ä¿å­˜é—´éš”


    bf16=True,
    report_to=["wandb"],
    run_name=run.name,
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset_small["train"],
    eval_dataset=dataset_small["valid"],
    args=training_args,
    formatting_func=formatting_prompts_func,
    data_collator=collator,
    max_seq_length=512,  # åºåˆ—æœ€å¤§é•¿åº¦
)

# =========================
# 6. å¼€å§‹è®­ç»ƒ
# =========================
try:
    print("ðŸš€ Starting LoRA SFT training...")
    trainer.train()

    # =========================
    # 7. æŽ¨ç†æµ‹è¯•
    # =========================
    print("\n--- æŽ¨ç†æµ‹è¯• ---")

    def test_model(idx=0):
        # ä½¿ç”¨å¤„ç†åŽçš„promptè¿›è¡Œæµ‹è¯•
        original_prompt = dataset_small["test"][idx]["prompt"]
        processed_prompt = dataset_small["test"][idx]["processed_prompt"]
        gt = dataset_small["test"][idx]["label"]

        inputs = tokenizer(processed_prompt, return_tensors="pt").to(device)

        model.eval()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=80,
                do_sample=False,
                eos_token_id=tokenizer.eos_token_id,
            )

        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        summary = decoded.split("TL;DR:")[-1].strip()

        print("\n[åŽŸå§‹Prompt]")
        print(original_prompt[-200:])
        print("\n[å¤„ç†åŽè¾“å…¥]")
        print(processed_prompt)
        print("\n[Humanæ‘˜è¦]")
        print(gt)
        print("\n[Modelç”Ÿæˆ]")
        print(summary)

    test_model(7)
    test_model(5)

    # =========================
    # 8. ä¿å­˜ LoRA Adapter
    # =========================
    trainer.save_model(f"{output_dir}/lora_adapter_final")
    print(f"âœ… æ¨¡åž‹å·²ä¿å­˜åˆ°: {output_dir}/lora_adapter_final")

except Exception as e:
    print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºçŽ°é”™è¯¯: {e}")
    print("è¯·æ£€æŸ¥é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®")

finally:
    # ç¡®ä¿wandbè¿žæŽ¥æ­£ç¡®å…³é—­
    wandb.finish()
