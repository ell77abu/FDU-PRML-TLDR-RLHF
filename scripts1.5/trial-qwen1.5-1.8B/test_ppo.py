# import os
# import torch
# import re
# from tqdm import tqdm
# from transformers import (
#     AutoTokenizer, 
#     AutoModelForSequenceClassification, 
#     BitsAndBytesConfig
# )
# from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
# from datasets import load_dataset
# from peft import LoraConfig

# # --- 1. åŸºç¡€é…ç½® ---
# sft_model_path = "./models/sft-tldr/final_checkpoint"
# rm_model_path = "./models/rm-tldr/final_rm" 

# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# device = 'cuda' if torch.cuda.is_available() else 'cpu'

# config = PPOConfig(
#     model_name="qwen-1.8b-ppo-lora-clean",
#     learning_rate=1e-8,          # LoRA å»ºè®®æ¯”å…¨å‚æ•°ç•¥é«˜
#     batch_size=16,               # 4090 æ˜¾å­˜é€‚é…
#     mini_batch_size=1,           
#     gradient_accumulation_steps=16, 
#     optimize_cuda_cache=True,
#     target_kl=0.1,               # é™åˆ¶åç¦» SFT å¤ªè¿œ
#     init_kl_coef=0.2,           # åˆå§‹ KL æƒ©ç½šï¼Œé˜²æ­¢æ¨¡åž‹ä¸€ä¸Šæ¥å°±ä¹±å†™
#     whiten_rewards=True,         
# )

# # --- 2. æ¨¡åž‹åŠ è½½ä¸Žæ˜¾å­˜åŽ‹ç¼© ---
# tokenizer = AutoTokenizer.from_pretrained(sft_model_path)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "left" 

# # LoRA é…ç½®ï¼šè¦†ç›–å…¨éƒ¨çº¿æ€§å±‚ä»¥ä¿è¯è¡¨è¾¾åŠ›
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=32,
#     target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
#     task_type="CAUSAL_LM",
#     bias="none",
# )

# # A. ç­–ç•¥ä¸Žä»·å€¼æ¨¡åž‹ (Policy & Value)ï¼šå…±ç”¨ LoRA åŸºåº§
# model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     sft_model_path,
#     peft_config=lora_config,
#     torch_dtype=torch.bfloat16,
#     device_map={"": device}
# )

# # B. å¥–åŠ±æ¨¡åž‹ (RM)ï¼šä½¿ç”¨ 4-bit é‡åŒ–ï¼Œå°†æ˜¾å­˜å ç”¨åŽ‹åˆ° 1.5GB å·¦å³
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,
#     bnb_4bit_compute_dtype=torch.bfloat16,
#     bnb_4bit_quant_type="nf4"
# )
# reward_model = AutoModelForSequenceClassification.from_pretrained(
#     rm_model_path,
#     quantization_config=bnb_config,
#     device_map={"": device}
# ).eval()

# # --- 3. å¥–åŠ±æ•´å½¢é€»è¾‘ (é’ˆå¯¹ä½ çš„ CSV é—®é¢˜å®šåˆ¶) ---
# def get_shaped_reward(raw_score, response_text):
#     penalty = 0.0
    
#     # æƒ©ç½š 1ï¼šè¡¨æƒ…åŒ…å †ç Œ (é’ˆå¯¹ :) :) :) :) )
#     # å¦‚æžœä»»æ„ç¬¦å·é‡å¤å‡ºçŽ° 3 æ¬¡ä»¥ä¸Šï¼Œé‡ç½š
#     if re.search(r'([:\)\!\?\.])\1{2,}', response_text):
#         penalty += 5.0
        
#     # æƒ©ç½š 2ï¼šæ•°æ®å™ªå£° (é’ˆå¯¹ "10 points for...", "SUBREDDIT:")
#     black_list = ["points for", "SUBREDDIT", "POST:", "TITLE:", "Thanks for", "Help!"]
#     for word in black_list:
#         if word.lower() in response_text.lower():
#             penalty += 10.0  # æ ¸å¿ƒæƒ©ç½šï¼šç¦æ­¢å¤è¯»åŽŸæ–‡æ ¼å¼
            
#     # æƒ©ç½š 3ï¼šé•¿åº¦å†—ä½™ (æ‘˜è¦åº”åœ¨ 15-50 è¯ä¹‹é—´)
#     word_count = len(response_text.split())
#     if word_count > 60:
#         penalty += (word_count - 60) * 0.2  # çº¿æ€§æƒ©ç½šé•¿æ–‡æœ¬
#     elif word_count < 5:
#         penalty += 2.0  # æƒ©ç½šè¿‡çŸ­

#     return raw_score - penalty

# # --- 4. æ•°æ®é¢„å¤„ç† ---
# dataset = load_dataset("CarperAI/openai_summarize_tldr", split="train")
# dataset = dataset.shuffle(seed=42).select(range(512))

# def tokenize_fn(sample):
#     # 1. å…ˆæ¸…ç†æŽ‰å¯èƒ½å­˜åœ¨çš„æœ«å°¾ç©ºæ ¼
#     query = sample["prompt"].rstrip() 
    
#     # 2. æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«äº† TL;DR: 
#     # å¦‚æžœæ²¡æœ‰ï¼Œæ‰æ·»åŠ ã€‚å¦‚æžœæœ‰ï¼Œç›´æŽ¥ä½¿ç”¨ã€‚
#     if not query.endswith("TL;DR:"):
#         query += "\nTL;DR:"
    
#     sample["query"] = query
#     # 3. ç¼–ç è½¬æ¢
#     sample["input_ids"] = tokenizer.encode(query, truncation=True, max_length=512)
#     return sample

# dataset = dataset.map(tokenize_fn, batched=False)
# dataset.set_format(type="torch")

# ppo_trainer = PPOTrainer(
#     config=config,
#     model=model,
#     ref_model=None, # PEFT æ¨¡å¼ä¸‹ä¼  None å¼€å¯æ˜¾å­˜å…±äº«æ¨¡å¼
#     tokenizer=tokenizer,
#     dataset=dataset,
#     data_collator=lambda data: {key: [d[key] for d in data] for key in data[0]},
# )

# # --- 5. è®­ç»ƒå¾ªçŽ¯ ---
# generation_kwargs = {
#     "do_sample": True,
#     "temperature": 0.8,         # ä¿æŒä½Žæ¸©ï¼Œå‡å°‘éšæœºä¹±ç 
#     "repetition_penalty": 1.5,  # å¼ºåŠ›åŽ‹åˆ¶åŽŸæ–‡é‡å¤
#     "max_new_tokens": 60,
#     "pad_token_id": tokenizer.pad_token_id,
# }

# print("ðŸš€ å¯åŠ¨çº åç‰ˆ LoRA-PPO è®­ç»ƒ...")

# for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
#     query_tensors = batch["input_ids"]

#     # ç”Ÿæˆ
#     response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
#     batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

#     # æ‰“åˆ†
#     tokenizer.padding_side = "right" 
#     texts = [q + r + tokenizer.eos_token for q, r in zip(batch["query"], batch["response"])]
#     inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    
#     with torch.no_grad():
#         raw_rewards = reward_model(**inputs).logits.squeeze(-1)
#         # åº”ç”¨é’ˆå¯¹æ€§æ•´å½¢
#         rewards = [get_shaped_reward(r.float(), resp) for r, resp in zip(raw_rewards, batch["response"])]

#     tokenizer.padding_side = "left" 

#     # æ›´æ–°
#     stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    
#     # æ—¥å¿—è¾“å‡º
#     if epoch % 1 == 0:
#         print(f"\nStep {epoch} | Reward: {torch.mean(torch.tensor(rewards)).item():.4f}")
#         print(f"Sample: {batch['response'][0][:150]}") # ç›‘æŽ§æ˜¯å¦æœ‰ :) :)

# # --- 6. ä¿å­˜ ---
# ppo_trainer.save_pretrained("./qwen-1.8b-ppo-lora-final")




import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, get_peft_model
import pandas as pd

# --- 1. é…ç½®è·¯å¾„ ---
model_path = "./models/sft-tldr/final_checkpoint"
device = "cuda" if torch.cuda.is_available() else "cpu"

# --- 2. åŠ è½½åŸºç¡€ SFT æ¨¡åž‹ ---
print("æ­£åœ¨åŠ è½½åŽŸå§‹ SFT æ¨¡åž‹...")
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.pad_token = tokenizer.eos_token
# å¿…é¡»ç»Ÿä¸€ padding ä¾§ï¼Œå¦åˆ™åˆ†å¸ƒä¼šæœ‰ç»†å¾®å·®å¼‚
tokenizer.padding_side = "left" 

model_sft = AutoModelForCausalLM.from_pretrained(
    model_path, 
    torch_dtype=torch.bfloat16,
    device_map={"": device}
).eval()

# --- 3. åˆ›å»ºå¸¦æœ‰åˆå§‹ LoRA çš„æ¨¡åž‹ ---
print("æ­£åœ¨æž„å»º SFT + LoRA æ¨¡åž‹...")
# ä½¿ç”¨ä½  PPO è„šæœ¬ä¸­å®Œå…¨ä¸€æ ·çš„é…ç½®
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    task_type="CAUSAL_LM",
    bias="none"
)
# get_peft_model ä¼šåœ¨åŽŸæ¨¡åž‹åŸºç¡€ä¸ŠåŒ…è£… LoRA å±‚
model_lora = get_peft_model(model_sft, lora_config).to(device).eval()

# --- 4. å‡†å¤‡æµ‹è¯•æ ·æœ¬ ---
test_samples = [
    "SUBREDDIT: r/relationships TITLE: Me [13 M] and my crush [12 F]. How do I ask her to the upcoming school dance? POST: Hey r/relationships! So this past Thursday my seventh grade class went on a school trip to Boston, and during this trip my crush ended up breaking up with her eighth grader boyfriend, I'll refer to him as Ian. Now I moved to this school this past year and Ian was my first friend, and what he ended up doing was dating my crush, Lily, so what happened on Thursday night is, according to Lily's friend, he sent her a picture of a pornstar in a quite revealing outfit, with a crude message something along the lines of \"if you wear this I'll f*** you\". She immediately broke up with him and things were quite awkward between them today as this was the first school day back. Now I've had a crush on Lily all year, but I found out Ian was dating her so I waited. Now she's free and seems to be over him, and is acting quite nice to me, which is very odd. Now the school dance is approaching in May, so I was curious if I should ask her, how, and when?  If I left anything out feel free to ask! TL;DR:",
    "SUBREDDIT: r/personalfinance TITLE: Prioritize student debt or saving for down payment? POST: I have $25k in student debt. One private loan at 9.5% (highest priority obviously) and nine others federal between 3.4% and 6.8%. Minimum payment per month total is $301.16. Over the next 9 months, I will pay off $11k of these, which will get rid of everything above 5% interest and will drop the total minimum payment to $150.   At the end of the 9 months, our savings will be around $35k. At that time my husband will need to purchase a car so some of that will be his down payment. So more realistically $25-30k.   Sometime in the future, between a year to two years from now, my husband and I may be moving. Typical single family homes in this area go for around $300k.   At the end of the 9 months, should I continue to focus on paying down student debt (which will be a balance of $14k by then) or growing our savings/down payment? I have $5200/mo to somehow split between debt and down payment and I'm not sure how best to allocate it. TL;DR:",
]

# --- 5. æ‰§è¡Œå¯¹æ¯”æŽ¨ç† ---
# ä½¿ç”¨å›ºå®šçš„ç”Ÿæˆå‚æ•°ï¼Œç¡®ä¿ç»“æžœå¯å¤çŽ°
gen_kwargs = {
    "max_new_tokens": 50,
    "do_sample": False,  # å¼ºåˆ¶è´ªå©ªæœç´¢ä»¥éªŒè¯æƒé‡ä¸€è‡´æ€§
    "repetition_penalty": 1.0,
    "pad_token_id": tokenizer.pad_token_id,
}

results = []

print("\nå¼€å§‹å¯¹æ¯”æŽ¨ç†...\n")
for i, prompt in enumerate(test_samples):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    
    with torch.no_grad():
        # åŽŸå§‹ SFT è¾“å‡º
        # æ³¨æ„ï¼šç”±äºŽ model_lora åŒ…è£…äº† model_sftï¼Œ
        # æˆ‘ä»¬éœ€è¦ç”¨ model_lora.disable_adapter() æ¥æ¨¡æ‹ŸåŽŸå§‹ SFT
        with model_lora.disable_adapter():
            out_sft = model_lora.generate(**inputs, **gen_kwargs)
            text_sft = tokenizer.decode(out_sft[0], skip_special_tokens=True)
        
        # åˆå§‹ LoRA è¾“å‡º
        out_lora = model_lora.generate(**inputs, **gen_kwargs)
        text_lora = tokenizer.decode(out_lora[0], skip_special_tokens=True)
    
    results.append({
        "ID": i + 1,
        "SFT_Output": text_sft.split("TL;DR:")[-1].strip(),
        "LoRA_Init_Output": text_lora.split("TL;DR:")[-1].strip(),
        "Match": text_sft == text_lora
    })

# --- 6. ç»“æžœå±•ç¤º ---
df = pd.DataFrame(results)
print(df.to_string())
# --- 7. é€»è¾‘åˆ¤å®š ---
if df["Match"].all():
    print("\nâœ… éªŒè¯é€šè¿‡ï¼šåˆå§‹ LoRA çŸ©é˜µå¯¹æ¨¡åž‹è¾“å‡ºæ²¡æœ‰ä»»ä½•å½±å“ï¼ˆ$\Delta W=0$ï¼‰ã€‚")
    print("è¿™æ„å‘³ç€ PPO åˆšå¼€å§‹æ—¶çš„å¤è¯»é—®é¢˜ä¸Ž LoRA ç»“æž„æ— å…³ï¼Œæ˜¯ç”±äºŽè®­ç»ƒè¶…å‚æ•°æˆ–æ›´æ–°å¯¼è‡´çš„ã€‚")
else:
    print("\nâŒ è­¦å‘Šï¼šä¸¤è€…è¾“å‡ºä¸ä¸€è‡´ï¼")
    print("å¯èƒ½åŽŸå› ï¼š1. LoRA åŒ…å«äº†éžé›¶åˆå§‹åŒ–çš„å±‚ï¼›2. æŸäº› target_modules åœ¨åŠ è½½æ—¶æ”¹å˜äº†åŽŸå§‹ç²¾åº¦ã€‚")