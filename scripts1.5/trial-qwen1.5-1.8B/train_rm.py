import os
import time
import torch
import wandb
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
)
from datasets import load_dataset
from trl import RewardTrainer

# --- 1. é…ç½® ---
model_id = "./models/sft-tldr/final_checkpoint"  # SFT æ¨¡å‹è·¯å¾„
output_dir = "./models/rm-tldr"
DEBUG = False 

TRAIN_SAMPLE = 20000 # åŸä¸º8000
EVAL_SAMPLE = 1000

# --- 0. åˆå§‹åŒ– Weights & Biases ---
run = wandb.init(
    project="qwen-rm-optimized",
    name=f"qwen-1.8b-rm-{int(time.time())}",
    config={
        "model_id": model_id,
        "train_sample": TRAIN_SAMPLE,
        "eval_sample": EVAL_SAMPLE,
        "learning_rate": 2e-5,
        "weight_decay": 0.1,
        "global_batch": 64,
        "max_seq_length": 1024,
    },
)

# --- 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.padding_side = "right" # å¿…é¡»å³å¡«å……
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=1,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False 

# --- 3. æ•°æ®å¤„ç† ---
dataset = load_dataset("openai/summarize_from_feedback", "comparisons")

if DEBUG:
    train_dataset_raw = dataset["train"].select(range(256))
    eval_dataset_raw = dataset["validation"].select(range(128))
else:
    # è®­ç»ƒé›†å’ŒéªŒè¯é›†éƒ½è¿›è¡Œ shuffleï¼Œç¡®ä¿åˆ†å¸ƒå‡åŒ€
    train_dataset_raw = dataset["train"].shuffle(seed=42).select(range(min(TRAIN_SAMPLE, len(dataset["train"]))))
    eval_dataset_raw = dataset["validation"].shuffle(seed=42).select(range(min(EVAL_SAMPLE, len(dataset["validation"]))))

def preprocess_function(examples):
    new_examples = {
        "input_ids_chosen": [],
        "attention_mask_chosen": [],
        "input_ids_rejected": [],
        "attention_mask_rejected": [],
    }
    for prompt, summaries, choice in zip(examples["info"], examples["summaries"], examples["choice"]):
        # æ›´åŠ æ¸…æ™°çš„ Prompt æ„é€ 
        # p = f"Post: {prompt['post']}\n\nTL;DR:"
        p = f"Post: {prompt['post']}\nTL;DR:"
        c = f" {summaries[choice]['text']}{tokenizer.eos_token}"
        r = f" {summaries[1 - choice]['text']}{tokenizer.eos_token}"

        # é€‚å½“è°ƒæ•´ max_lengthï¼Œç¡®ä¿æ‘˜è¦ä¸è¢«æˆªæ–­è¿‡å¤š
        tokenized_chosen = tokenizer(p + c, max_length=1024, truncation=True)
        tokenized_rejected = tokenizer(p + r, max_length=1024, truncation=True)

        new_examples["input_ids_chosen"].append(tokenized_chosen["input_ids"])
        new_examples["attention_mask_chosen"].append(tokenized_chosen["attention_mask"])
        new_examples["input_ids_rejected"].append(tokenized_rejected["input_ids"])
        new_examples["attention_mask_rejected"].append(tokenized_rejected["attention_mask"])

    return new_examples

train_dataset = train_dataset_raw.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)
eval_dataset = eval_dataset_raw.map(preprocess_function, batched=True, remove_columns=dataset["validation"].column_names)

# --- 4. Data Collator ---
class RewardDataCollator:
    def __call__(self, features):
        batch = {}
        for k in ["chosen", "rejected"]:
            inputs = [{"input_ids": f[f"input_ids_{k}"], "attention_mask": f[f"attention_mask_{k}"]} for f in features]
            padded = tokenizer.pad(inputs, padding=True, return_tensors="pt")
            batch[f"input_ids_{k}"] = padded["input_ids"]
            batch[f"attention_mask_{k}"] = padded["attention_mask"]
        return batch

# --- 5. è®­ç»ƒå‚æ•°è®¾ç½® (é’ˆå¯¹ 4090 æ·±åº¦ä¼˜åŒ–) ---
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,   
    gradient_accumulation_steps=32,  # æ€» Batch Size = 64
    
    # ç­–ç•¥è°ƒæ•´ï¼šé’ˆå¯¹ 1.8B æ¨¡å‹æé«˜å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–
    learning_rate=2e-5,              
    lr_scheduler_type="linear",      # çº¿æ€§è¡°å‡åœ¨å°æ¨¡å‹ä¸Šæ›´ç¨³å®š
    warmup_steps=100,                # å›ºå®šçš„çƒ­èº«æ­¥æ•°
    weight_decay=0.1,                # å¼ºåŒ–æ­£åˆ™åŒ–ï¼Œé˜²æ­¢ 1.8B åç¼©
    max_grad_norm=1.0,               
    
    num_train_epochs=1,              
    bf16=True,                      
    gradient_checkpointing=True,     
    gradient_checkpointing_kwargs={"use_reentrant": False},
    
    # å¼ºåŒ–ç›‘æ§ï¼šæ¯ 20 æ­¥éªŒè¯ä¸€æ¬¡
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=40,                   
    save_strategy="steps",
    save_steps=40,
    
    # è‡ªåŠ¨ä¿å­˜å¹¶åŠ è½½æœ€ä½³æ¨¡å‹
    load_best_model_at_end=True,     
    metric_for_best_model="accuracy",
    greater_is_better=True,
    save_total_limit=2,              # èŠ‚çœç£ç›˜ç©ºé—´
    
    remove_unused_columns=False,
    report_to=["wandb"],            
)

# --- 6. å¯åŠ¨è®­ç»ƒ ---
trainer = RewardTrainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=RewardDataCollator(),
)



print("ğŸš€ å¼€å§‹ä¼˜åŒ–åçš„ Reward Model è®­ç»ƒ...")
trainer.train()

# --- 7. ä¿å­˜ç»“æœ ---
trainer.save_model(f"{output_dir}/final_rm")
tokenizer.save_pretrained(f"{output_dir}/final_rm")
print(f"âœ… æœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {output_dir}/final_rm")