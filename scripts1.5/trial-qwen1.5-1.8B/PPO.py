# import torch
# from tqdm import tqdm
# from transformers import AutoTokenizer
# from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
# from datasets import load_dataset

# # --- 1. é…ç½® ---
# model_id = "./sft-tldr/final_checkpoint" # SFT æ¨¡å‹è·¯å¾„
# rm_model_id = "./rm-tldr/final_rm"       # è®­ç»ƒå¥½çš„ RM è·¯å¾„
# device = "cuda" if torch.cuda.is_available() else "cpu"

# config = PPOConfig(
#     model_name="qwen-1.8b-ppo",
#     learning_rate=1.41e-5,
#     batch_size=32,          # æ¯æ¬¡ PPO æ›´æ–°ä½¿ç”¨çš„æ ·æœ¬æ€»æ•°
#     mini_batch_size=4,      # æ˜¾å­˜é™åˆ¶ä¸‹çš„å¾®æ‰¹æ¬¡
#     gradient_accumulation_steps=8,
#     optimize_cuda_cache=True,
#     early_stopping=False,
#     target_kl=0.1,          # ç›®æ ‡ KL æ•£åº¦
#     init_kl_coeff=0.1,      # åˆå§‹ KL æƒ©ç½šç³»æ•°
#     adap_kl_ctrl=True,      # åŠ¨æ€è°ƒæ•´ KL
# )

# # --- 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
# # æ³¨æ„ï¼šPPO ç”Ÿæˆé€šå¸¸ç”¨ left padding
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "left" 

# # åŠ è½½å¸¦ Value Head çš„ç­–ç•¥æ¨¡å‹
# model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     model_id, 
#     torch_dtype=torch.bfloat16,
#     trust_remote_code=True
# ).to(device)

# # åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“çš„ SFTï¼Œç”¨äºè®¡ç®— KLï¼‰
# ref_model = create_reference_model(model)

# # åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰
# from transformers import AutoModelForSequenceClassification
# reward_model = AutoModelForSequenceClassification.from_pretrained(
#     rm_model_id, 
#     torch_dtype=torch.bfloat16,
#     trust_remote_code=True
# ).to(device).eval()

# # --- 3. æ•°æ®å¤„ç† ---
# dataset = load_dataset("openai/summarize_from_feedback", "comparisons", split="train")
# dataset = dataset.shuffle(seed=42).select(range(5000)) # PPO è¿­ä»£é€šå¸¸ä¸éœ€è¦æµ·é‡æ•°æ®

# def tokenize(sample):
#     # ä¸¥æ ¼å¯¹é½è®­ç»ƒæ—¶çš„ Prompt æ ¼å¼
#     prompt = f"Post: {sample['info']['post']}\nTL;DR:"
#     sample["input_ids"] = tokenizer.encode(prompt, truncation=True, max_length=512)
#     sample["query"] = prompt
#     return sample

# dataset = dataset.map(tokenize, batched=False)
# dataset.set_format(type="torch")

# def collator(data):
#     return {key: [d[key] for d in data] for key in data[0]}

# # --- 4. åˆå§‹åŒ– PPO Trainer ---
# ppo_trainer = PPOTrainer(
#     config=config,
#     model=model,
#     ref_model=ref_model,
#     tokenizer=tokenizer,
#     dataset=dataset,
#     data_collator=collator,
# )

# # --- 5. è®­ç»ƒå¾ªç¯ ---
# generation_kwargs = {
#     "min_length": -1,
#     "top_k": 0.0,
#     "top_p": 1.0,
#     "do_sample": True,
#     "pad_token_id": tokenizer.pad_token_id,
#     "max_new_tokens": 100, # æ‘˜è¦é•¿åº¦æ§åˆ¶
# }



# print("ğŸš€ å¼€å§‹ PPO è®­ç»ƒ...")
# for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
#     query_tensors = batch["input_ids"]

#     # A. Policy æ¨¡å‹ç”Ÿæˆæ‘˜è¦
#     response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
#     batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

#     # B. æ„é€  RM çš„è¾“å…¥å¹¶æ‰“åˆ†
#     # æ³¨æ„ï¼šRM è¯„ä¼°æ—¶é€šå¸¸éœ€è¦å¯¹é½è®­ç»ƒæ—¶çš„æ‹¼æ¥æ ¼å¼
#     texts = [q + r + tokenizer.eos_token for q, r in zip(batch["query"], batch["response"])]
    
#     # åˆ‡æ¢åˆ° RM éœ€è¦çš„å³å¡«å……æ¨¡å¼è¿›è¡Œæ¨ç†ï¼ˆä¸´æ—¶æ“ä½œï¼‰
#     tokenizer.padding_side = "right"
#     inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
#     with torch.no_grad():
#         rewards = reward_model(**inputs).logits.squeeze(-1)
    
#     # å¥–åŠ±å½’ä¸€åŒ–å¤„ç†ï¼ˆå¯é€‰ï¼šå‡å»å‡å€¼ï¼Œè®©å¥–åŠ±æœ‰æ­£æœ‰è´Ÿï¼‰
#     rewards = [torch.tensor(r) for r in rewards]
#     tokenizer.padding_side = "left" # åˆ‡å›ç”Ÿæˆæ¨¡å¼

#     # C. æ‰§è¡Œ PPO æ­¥è¿›
#     stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    
#     # D. æ‰“å°å…³é”®æŒ‡æ ‡
#     if epoch % 10 == 0:
#         ppo_trainer.log_stats(stats, batch, rewards)
#         print(f"Epoch {epoch} | Mean Reward: {stats['ppo/returns/mean']:.4f} | Mean KL: {stats['objective/kl']:.4f}")

# # --- 6. ä¿å­˜æ¨¡å‹ ---
# ppo_trainer.save_pretrained("./ppo-tldr-final")
# print("âœ… PPO è®­ç»ƒå®Œæˆå¹¶ä¿å­˜ï¼")

# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM
# from peft import LoraConfig, get_peft_model

# model_path = "./models/sft-tldr/final_checkpoint"
# device = "cuda" if torch.cuda.is_available() else "cpu"

# # 1. åŠ è½½åŸå§‹ SFT æ¨¡å‹
# tokenizer = AutoTokenizer.from_pretrained(model_path)
# model_sft = AutoModelForCausalLM.from_pretrained(
#     model_path, 
#     torch_dtype=torch.bfloat16
# ).to(device)

# # 2. å‡†å¤‡æµ‹è¯•æ•°æ®
# test_prompt = "SUBREDDIT: r/relationships\nTITLE: My boyfriend is depressed...\nPOST: [æ­¤å¤„çœç•¥åŸæ–‡å†…å®¹]\nTL;DR:"
# inputs = tokenizer(test_prompt, return_tensors="pt").to(device)

# # 3. åŸå§‹ SFT ç”Ÿæˆ
# with torch.no_grad():
#     out_sft = model_sft.generate(**inputs, max_new_tokens=30, do_sample=False)
#     text_sft = tokenizer.decode(out_sft[0], skip_special_tokens=True)

# # 4. æŒ‚è½½ LoRA (æ¨¡æ‹Ÿ PPO åˆšå¼€å§‹çš„çŠ¶æ€)
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=32,
#     target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
#     task_type="CAUSAL_LM",
# )
# model_lora = get_peft_model(model_sft, lora_config)

# # 5. LoRA ç‰ˆæœ¬ç”Ÿæˆ
# with torch.no_grad():
#     out_lora = model_lora.generate(**inputs, max_new_tokens=50, do_sample=False)
#     text_lora = tokenizer.decode(out_lora[0], skip_special_tokens=True)

# print(f"\n--- åŸå§‹ SFT è¾“å‡º ---\n{text_sft}")
# print(f"\n--- æŒ‚è½½ LoRA åè¾“å‡º ---\n{text_lora}")

# # 6. é€»è¾‘æ£€æŸ¥
# is_same = text_sft == text_lora
# print(f"\nç»“è®ºï¼šä¸¤è€…è¾“å‡ºæ˜¯å¦ä¸€è‡´? {'âœ… æ˜¯' if is_same else 'âŒ å¦'}")





import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
from datasets import load_dataset
from tqdm import tqdm
import pandas as pd

# --- 1. é…ç½®è·¯å¾„ ---
sft_model_path = "./models/sft-tldr/final_checkpoint"
device = "cuda" if torch.cuda.is_available() else "cpu"

# --- 2. åŠ è½½æ¨¡å‹ä¸ Tokenizer ---
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
tokenizer = AutoTokenizer.from_pretrained(sft_model_path)
# SFT æ¨¡å‹ç”¨äºç”Ÿæˆæ‘˜è¦
sft_model = AutoModelForCausalLM.from_pretrained(
    sft_model_path, torch_dtype=torch.bfloat16
).to(device).eval()


# --- 3. åŠ è½½æµ‹è¯•æ•°æ®é›† ---
dataset = load_dataset("openai/summarize_from_feedback", "comparisons")
# é€‰å– 50 æ¡æ•°æ®è¿›è¡Œå¯¹æ¯”æµ‹è¯•
test_samples = dataset["validation"].shuffle(seed=42).select(range(10))

results = []

print("å¼€å§‹ç”Ÿæˆ...")
for i, sample in enumerate(tqdm(test_samples)):
    prompt = f"Post: {sample['info']['post']}\nTL;DR:"
    human_summary = f" {sample['summaries'][sample['choice']]['text']}{tokenizer.eos_token}"
    
    # --- Step A: SFT æ¨¡å‹ç”Ÿæˆæ‘˜è¦ ---
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = sft_model.generate(
            **inputs, 
            max_new_tokens=80, 
            # do_sample=True, 
            do_sample=False, 
            # temperature=0.8,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id
        )
    # æå–ç”Ÿæˆçš„æ‘˜è¦éƒ¨åˆ†
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    sft_summary = full_text.split("TL;DR:")[-1].strip()
    print(f"\næ ·æœ¬ {i+1}:\nHuman: {human_summary}\nSFT: {sft_summary}")