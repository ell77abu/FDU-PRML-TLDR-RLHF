import os
import time
import torch
import wandb
from datasets import load_dataset
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    TrainingArguments, 
    set_seed
)
from peft import LoraConfig, get_peft_model, TaskType
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# è®¾ç½®éšæœºç§å­ä¿è¯å¯å¤ç°æ€§
set_seed(42)

# =========================
# 1. ç¯å¢ƒä¸åŸºæœ¬é…ç½®
# =========================
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
device = 'cuda' if torch.cuda.is_available() else 'cpu'

model_id = './models/Qwen1.5-1.8B'
output_dir = "./sft-tldr-lora-improved"

# æ›´åŠ ç§‘å­¦çš„è¶…å‚æ•°é…ç½®
config = {
    "train_sample": 50000,
    "eval_sample": 2000,
    "learning_rate": 1e-4,       # å¯¹äº LoRAï¼Œ1e-4 é€šå¸¸æ¯” 5e-5 æ”¶æ•›æ›´å¿«æ›´ç¨³
    "num_train_epochs": 3,
    "per_device_batch_size": 4,  # 1.8B æ¨¡å‹è¾ƒå°ï¼Œå¯é€‚å½“å¢åŠ  BS
    "grad_accum": 4,             # å…¨å±€ Batch Size = 4 * 4 = 16
    "max_seq_length": 512,
    "lora_r": 32,                # å¢åŠ åˆ° 32 æå‡ 1.8B å°æ¨¡å‹çš„æ‹Ÿåˆèƒ½åŠ›
    "lora_alpha": 64,            # é€šå¸¸ä¸º r çš„ 2 å€
}

run = wandb.init(
    project="prml-sft-lora-improved",
    name=f"qwen1.5-1.8b-sft-{int(time.time())}",
    config=config,
)

# =========================
# 2. Tokenizer (ä¿®æ­£ Pad Token é—®é¢˜)
# =========================
tokenizer = AutoTokenizer.from_pretrained(
    model_id,
    use_fast=False,
    trust_remote_code=True,
    local_files_only=True,
)

# Qwen1.5 é»˜è®¤æ²¡æœ‰ pad_tokenï¼Œä½¿ç”¨ eos_token å¡«å……
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right" 

# =========================
# 3. æ•°æ®é›†ä¸è¿‡æ»¤ (é€»è¾‘ä¼˜åŒ–)
# =========================
dataset = load_dataset("CarperAI/openai_summarize_tldr")

def preprocess_and_filter(example):
    # æå–å†…å®¹
    if "POST:" in example['prompt'] and "TL;DR:" in example['prompt']:
        post_start = example['prompt'].find("POST:") + 5
        tldr_start = example['prompt'].find("TL;DR:")
        post_content = example['prompt'][post_start:tldr_start].strip()
        # ä½¿ç”¨TL;DRæ ¼å¼ï¼Œä¸RMè®­ç»ƒå’ŒPPOè®­ç»ƒä¿æŒä¸€è‡´
        example['processed_prompt'] = f"{post_content}\nTL;DR:"
    else:
        example['processed_prompt'] = example['prompt']

    # è´¨é‡è¿‡æ»¤æ¡ä»¶
    prompt_len = len(example['processed_prompt'].split())
    label_len = len(example['label'].split())
    
    keep = (30 < prompt_len < 400 and 
            5 < label_len < 60 and 
            label_len / prompt_len < 0.6)
    return keep

print(f"åŸå§‹è®­ç»ƒé›†å¤§å°: {len(dataset['train'])}")
dataset = dataset.filter(preprocess_and_filter)
print(f"è¿‡æ»¤åè®­ç»ƒé›†å¤§å°: {len(dataset['train'])}")

# æŠ½æ ·
dataset_small = {
    "train": dataset["train"].select(range(min(config["train_sample"], len(dataset["train"])))),
    "valid": dataset["valid"].select(range(min(config["eval_sample"], len(dataset["valid"])))),
    "test": dataset["test"].select(range(min(100, len(dataset["test"])))),
}

def formatting_prompts_func(example):
    texts = []
    for i in range(len(example["processed_prompt"])):
        # ç¡®ä¿ prompt å’Œ label ä¹‹é—´æœ‰æ¸…æ™°çš„ç•Œé™
        text = f"{example['processed_prompt'][i]} {example['label'][i]}{tokenizer.eos_token}"
        texts.append(text)
    return texts

# ä½¿ç”¨TL;DRä½œä¸ºresponse templateï¼Œä¸æ•°æ®æ ¼å¼ä¿æŒä¸€è‡´
response_template = "TL;DR:"
collator = DataCollatorForCompletionOnlyLM(
    response_template=response_template,
    tokenizer=tokenizer,
)

# =========================
# 4. æ¨¡å‹ä¸ LoRA é…ç½® (æ‰©å±•ç›®æ ‡æ¨¡å—)
# =========================
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16, # Qwen1.5 æ¨è bf16
    device_map="auto",
    trust_remote_code=True,
    local_files_only=True,
)

lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=config["lora_r"],
    lora_alpha=config["lora_alpha"],
    lora_dropout=0.05,
    bias="none",
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj", # åŒ…å« MLP æ•ˆæœæ›´å¥½
    ],
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# =========================
# 5. è®­ç»ƒå‚æ•° (æ€§èƒ½ä¼˜åŒ–)
# =========================
training_args = TrainingArguments(
    output_dir=f"{output_dir}-final",
    per_device_train_batch_size=config["per_device_batch_size"],
    gradient_accumulation_steps=config["grad_accum"],
    gradient_checkpointing=False,         # 1.8B æ¨¡å‹æ˜¾å­˜è¶³å¤Ÿï¼Œå…³é—­å¯æé€Ÿçº¦ 20-30%
    
    learning_rate=config["learning_rate"],
    num_train_epochs=config["num_train_epochs"],
    weight_decay=0.01,
    warmup_ratio=0.1,
    lr_scheduler_type="cosine",

    logging_steps=10,
    eval_strategy="steps",
    eval_steps=200,
    save_strategy="steps",
    save_steps=400,
    save_total_limit=2,

    bf16=True,
    tf32=True,                            # å¦‚æœæ˜¯ Ampere æ¶æ„ï¼ˆå¦‚ 3090/A100ï¼‰å»ºè®®å¼€å¯
    report_to=["wandb"],
    run_name=run.name,
    remove_unused_columns=False,          # é…åˆ SFTTrainer ä½¿ç”¨
)

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset_small["train"],
    eval_dataset=dataset_small["valid"],
    args=training_args,
    formatting_func=formatting_prompts_func,
    data_collator=collator,
    max_seq_length=config["max_seq_length"],
    packing=False,                        # è®¾ä¸º False æ‰èƒ½è®© DataCollator å‡†ç¡® Mask æ‰ Prompt
)

# =========================
# 6. è®­ç»ƒä¸æµ‹è¯•
# =========================
try:
    print("ğŸš€ Starting Training...")
    trainer.train()

    print("\n--- æ¨ç†æµ‹è¯• ---")
    def test_model(idx=0):
        item = dataset_small["test"][idx]
        prompt = item["processed_prompt"]
        gt = item["label"]

        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        model.eval()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                eos_token_id=tokenizer.eos_token_id,
            )

        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æå– TL;DR: ä¹‹åçš„å†…å®¹
        summary = decoded.split("TL;DR:")[-1].strip()

        print(f"\n[Test Item {idx}]")
        print(f"Input: {prompt[:150]}...")
        print(f"Ground Truth: {gt}")
        print(f"Model Generated: {summary}")

    for i in [0, 1]: test_model(i)

    # ä¿å­˜
    trainer.save_model(f"{output_dir}/lora_adapter_final")
    print(f"âœ… Saved to: {output_dir}/lora_adapter_final")

except Exception as e:
    print(f"âŒ Error: {e}")
finally:
    wandb.finish()