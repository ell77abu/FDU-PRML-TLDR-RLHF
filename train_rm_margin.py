import os
import time
import torch
import torch.nn as nn
import wandb
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification, 
    TrainingArguments, 
)
from datasets import load_dataset
from trl import RewardTrainer

# --- 1. é…ç½® ---
model_id = "./models/sft-tldr/final_checkpoint"
output_dir = "./models/rm-tldr-margin-final"
DEBUG = False 

TRAIN_SAMPLE = 20000 
EVAL_SAMPLE = 1000

# --- 0. åˆå§‹åŒ– Weights & Biases ---
wandb.init(
    project="qwen-rm-optimized",
    name=f"qwen-1.8b-margin-rm-final",
    config={
        "learning_rate": 1e-5,
        "margin": 1.0,
        "train_sample": TRAIN_SAMPLE,
    },
)

# --- 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.padding_side = "right" 
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=1,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
)

model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False 

# --- 3. è‡ªå®šä¹‰ Trainerï¼šä¿®å¤ Eval IndexError å’Œ Accuracy é—®é¢˜ ---
class MarginRewardTrainer(RewardTrainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        margin = 1.0
        # è®¡ç®—å¥–åŠ±
        rewards_chosen = model(
            input_ids=inputs["input_ids_chosen"],
            attention_mask=inputs["attention_mask_chosen"],
        )[0]
        rewards_rejected = model(
            input_ids=inputs["input_ids_rejected"],
            attention_mask=inputs["attention_mask_rejected"],
        )[0]

        # Margin Loss
        loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected - margin).mean()

        if return_outputs:
            # ä¾›è®­ç»ƒç›‘æ§ä½¿ç”¨
            stacked_logits = torch.cat([rewards_chosen, rewards_rejected], dim=1).detach()
            return loss, {"logits": stacked_logits}
        return loss

    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):
        """
        æ ¸å¿ƒä¿®æ­£ï¼šé‡å†™é¢„æµ‹æ­¥ã€‚ç¡®ä¿åœ¨è¯„ä¼°å¾ªç¯ä¸­ï¼Œè¿”å›çš„ logits å½¢çŠ¶ä¸º (batch_size, 2)ã€‚
        """
        device = model.device
        with torch.no_grad():
            # å‡†å¤‡è¾“å…¥
            inputs = {k: v.to(device) for k, v in inputs.items() if isinstance(v, torch.Tensor)}
            
            # è®¡ç®— Loss
            loss = self.compute_loss(model, inputs, return_outputs=False)
            if prediction_loss_only:
                return (loss, None, None)

            # æ˜¾å¼è®¡ç®—ä¸¤ä¸ªåˆ†æ•°å¹¶æ‹¼æ¥
            r_chosen = model(input_ids=inputs["input_ids_chosen"], attention_mask=inputs["attention_mask_chosen"])[0]
            r_rejected = model(input_ids=inputs["input_ids_rejected"], attention_mask=inputs["attention_mask_rejected"])[0]
            
            # å½¢çŠ¶å¿…é¡»ä¸º (batch_size, 2)
            logits = torch.cat([r_chosen, r_rejected], dim=1).detach()
            # ä¼ªé€  labelsï¼ŒTRL å†…éƒ¨è®¡ç®— accuracy æ—¶å¹¶ä¸ä½¿ç”¨ label_ids
            labels = torch.zeros(logits.shape[0], device=device)

        return (loss, logits, labels)

# --- 4. æ•°æ®å¤„ç† ---
dataset = load_dataset("openai/summarize_from_feedback", "comparisons")

if DEBUG:
    train_dataset_raw = dataset["train"].select(range(256))
    eval_dataset_raw = dataset["validation"].select(range(128))
else:
    train_dataset_raw = dataset["train"].shuffle(seed=42).select(range(min(TRAIN_SAMPLE, len(dataset["train"]))))
    eval_dataset_raw = dataset["validation"].shuffle(seed=42).select(range(min(EVAL_SAMPLE, len(dataset["validation"]))))

def preprocess_function(examples):
    new_examples = {
        "input_ids_chosen": [], "attention_mask_chosen": [],
        "input_ids_rejected": [], "attention_mask_rejected": [],
    }
    for prompt, summaries, choice in zip(examples["info"], examples["summaries"], examples["choice"]):
        if summaries[0]['text'] == summaries[1]['text']:
            continue
        p = f"Post: {prompt['post']}\nTL;DR:"
        c = f" {summaries[choice]['text']}{tokenizer.eos_token}"
        r = f" {summaries[1 - choice]['text']}{tokenizer.eos_token}"

        t_chosen = tokenizer(p + c, max_length=1024, truncation=True)
        t_rejected = tokenizer(p + r, max_length=1024, truncation=True)

        new_examples["input_ids_chosen"].append(t_chosen["input_ids"])
        new_examples["attention_mask_chosen"].append(t_chosen["attention_mask"])
        new_examples["input_ids_rejected"].append(t_rejected["input_ids"])
        new_examples["attention_mask_rejected"].append(t_rejected["attention_mask"])
    return new_examples

train_dataset = train_dataset_raw.map(preprocess_function, batched=True, remove_columns=dataset["train"].column_names)
eval_dataset = eval_dataset_raw.map(preprocess_function, batched=True, remove_columns=dataset["validation"].column_names)

# --- 5. Data Collator ---
class RewardDataCollator:
    def __call__(self, features):
        batch = {}
        for k in ["chosen", "rejected"]:
            inputs = [{"input_ids": f[f"input_ids_{k}"], "attention_mask": f[f"attention_mask_{k}"]} for f in features]
            padded = tokenizer.pad(inputs, padding=True, return_tensors="pt")
            batch[f"input_ids_{k}"] = padded["input_ids"]
            batch[f"attention_mask_{k}"] = padded["attention_mask"]
        return batch

# --- 6. è®­ç»ƒå‚æ•°è®¾ç½® ---
training_args = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,   
    gradient_accumulation_steps=32,
    per_device_eval_batch_size=4,    # è°ƒå°ä»¥é˜²æ­¢ Eval æ—¶ OOM
    
    learning_rate=1e-5,              
    lr_scheduler_type="cosine",      
    warmup_ratio=0.1,                
    weight_decay=0.1,                
    max_grad_norm=1.0,               
    
    num_train_epochs=1,              
    bf16=True,                      
    gradient_checkpointing=True,     
    gradient_checkpointing_kwargs={"use_reentrant": False},
    
    logging_steps=15,
    evaluation_strategy="steps",
    eval_steps=50,                   
    save_strategy="steps",
    save_steps=50,
    
    load_best_model_at_end=True,     
    metric_for_best_model="accuracy",
    greater_is_better=True,
    save_total_limit=1,              
    remove_unused_columns=False,
    report_to=["wandb"],            
)

# --- 7. å¯åŠ¨è®­ç»ƒ ---
trainer = MarginRewardTrainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=RewardDataCollator(),
)

print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨ã€‚è®­ç»ƒé›†å¤§å°: {len(train_dataset)}ã€‚ä¿®å¤äº† Eval ç´¢å¼•é—®é¢˜ã€‚")
trainer.train()

# --- 8. ä¿å­˜ ---
trainer.save_model(f"{output_dir}/final_rm")
tokenizer.save_pretrained(f"{output_dir}/final_rm")
print(f"âœ… å®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨ {output_dir}/final_rm")