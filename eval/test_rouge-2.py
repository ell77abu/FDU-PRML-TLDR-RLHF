"""
ROUGE è¯„ä¼°è„šæœ¬ v2.0
è¯„ä¼°æ¨¡å‹åœ¨ TL;DR æ•°æ®é›†ä¸Šçš„æ‘˜è¦è´¨é‡
æ–°å¢ï¼šå¹³å‡é•¿åº¦ç»Ÿè®¡ã€å¤§è§„æ¨¡æ ·æœ¬æ”¯æŒã€æ˜¾å­˜ä¼˜åŒ–
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import json
import csv
import numpy as np
from datetime import datetime
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
import evaluate

# =========================================================
# é…ç½®
# =========================================================

MODELS_TO_EVALUATE = [
    {"name": "Base-Qwen3-1.7B", "path": "../models/Qwen3-1.7B"},
    {"name": "SFT-Full", "path": "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint"},
    {"name": "grpo-baseline", "path": "/workspace/pj-RL/experiments3/qwen3-grpo-final-10k/checkpoint-1500"},
    {"name": "grpo-axis", "path": "/workspace/pj-RL/experiments3/qwen3-grpo-axis-merged/checkpoint-1200-merged"},
    {"name": "grpo-hybrid", "path": "/workspace/pj-RL/qwen3-grpo-hybrid-rm/checkpoint-1950"},    
]

DATASET_PATH = "/workspace/pj-RL/datasets/openai_summarize_tldr"
TEST_SAMPLES = 500  # å·²ä¿®æ”¹ä¸º 500 æ¡

GENERATION_CONFIG = {
    "max_new_tokens": 80, # ç¨å¾®å¢åŠ ä¸Šé™ï¼Œç»™é•¿æ‘˜è¦ç•™å‡ºç©ºé—´
    "do_sample": True,
    "top_p": 0.9,
    "temperature": 0.8,
    "repetition_penalty": 1.1,
}

RESULTS_DIR = "./rouge_results"
os.makedirs(RESULTS_DIR, exist_ok=True)

# =========================================================
# å‡½æ•°å®šä¹‰
# =========================================================

def extract_post_only(prompt: str) -> str:
    """æå–POSTå†…å®¹ï¼Œç¡®ä¿æ¨ç†å¼•å¯¼è¯æ ¼å¼æ ‡å‡†"""
    if "POST:" in prompt:
        prompt = prompt.split("POST:", 1)[1]
    
    if "TL;DR:" in prompt:
        post, _ = prompt.split("TL;DR:", 1)
        prompt = post.strip() + "\n\nTL;DR: " # å¢åŠ ç©ºæ ¼å¼•å¯¼ç”Ÿæˆ
    else:
        prompt = prompt.strip() + "\n\nTL;DR: "
    
    return prompt

def evaluate_model_rouge(model, tokenizer, test_dataset, device):
    print("ğŸ“‹ æ­£åœ¨åˆå§‹åŒ– ROUGE è¯„ä¼°å™¨...")
    try:
        rouge = evaluate.load("rouge")
    except Exception as e:
        print(f"âŒ ROUGE åŠ è½½å¤±è´¥: {e}")
        raise e

    model.eval()
    predictions = []
    references = []
    gen_lengths = [] # ç”¨äºç»Ÿè®¡ token é•¿åº¦
    
    print(f"ğŸš€ Generating summaries for {len(test_dataset)} samples...")
    
    for example in tqdm(test_dataset):
        raw_prompt = example["prompt"]
        reference_summary = example["label"]
        clean_prompt = extract_post_only(raw_prompt)
        
        inputs = tokenizer(
            clean_prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **GENERATION_CONFIG,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # ç»Ÿè®¡ç”Ÿæˆçš„é•¿åº¦ (ä¸å« Prompt)
        prompt_len = inputs.input_ids.shape[1]
        generated_tokens = outputs[0][prompt_len:]
        gen_lengths.append(len(generated_tokens))
        
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # æå– TL;DR: ä¹‹åçš„éƒ¨åˆ†
        if "TL;DR:" in full_text:
            predicted_summary = full_text.split("TL;DR:")[-1].strip()
        else:
            predicted_summary = full_text.strip()
        
        predictions.append(predicted_summary)
        references.append(reference_summary)
    
    print("ğŸ“Š Computing ROUGE scores...")
    rouge_results = rouge.compute(
        predictions=predictions, 
        references=references,
        use_stemmer=True
    )
    
    avg_len = np.mean(gen_lengths)
    return rouge_results, avg_len, predictions, references

# =========================================================
# ä¸»å‡½æ•°
# =========================================================

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    print(f"Loading dataset: {DATASET_PATH}")
    dataset = load_from_disk(DATASET_PATH)
    # å¢åŠ å®‰å…¨æ€§æ£€æŸ¥ï¼Œé˜²æ­¢æ ·æœ¬æ•°æº¢å‡º
    max_test = len(dataset["test"])
    num_samples = min(TEST_SAMPLES, max_test)
    test_dataset = dataset["test"].select(range(num_samples))
    print(f"Loaded {num_samples} test samples")
    
    all_summary_results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for model_config in MODELS_TO_EVALUATE:
        model_name = model_config["name"]
        model_path = model_config["path"]
        
        print(f"\n{'='*20} Evaluating: {model_name} {'='*20}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )
        
        rouge_scores, avg_gen_len, _, _ = evaluate_model_rouge(
            model, tokenizer, test_dataset, device
        )
        
        print(f"\nâœ¨ {model_name} Results:")
        print(f"  ROUGE-1: {rouge_scores['rouge1']:.4f}")
        print(f"  ROUGE-2: {rouge_scores['rouge2']:.4f}")
        print(f"  ROUGE-L: {rouge_scores['rougeL']:.4f}")
        print(f"  Avg Gen Length: {avg_gen_len:.2f} tokens")
        
        # è®°å½•ç»“æœ
        res = {
            "model_name": model_name,
            "rouge1": rouge_scores['rouge1'],
            "rouge2": rouge_scores['rouge2'],
            "rougeL": rouge_scores['rougeL'],
            "avg_len": avg_gen_len,
            "timestamp": timestamp
        }
        all_summary_results.append(res)
        
        # ä¿å­˜å•ä¸ªæ¨¡å‹çš„è¯¦ç»† JSON
        json_path = f"{RESULTS_DIR}/{model_name}_{timestamp}.json"
        with open(json_path, "w") as f:
            json.dump(res, f, indent=2)

        # å½»åº•é‡Šæ”¾æ˜¾å­˜
        del model
        del tokenizer
        torch.cuda.empty_cache()
        import gc
        gc.collect()
    
    # ä¿å­˜å¯¹æ¯”æ±‡æ€» CSV
    csv_path = f"{RESULTS_DIR}/final_comparison_{timestamp}.csv"
    with open(csv_path, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["model_name", "rouge1", "rouge2", "rougeL", "avg_len", "timestamp"])
        writer.writeheader()
        writer.writerows(all_summary_results)
    
    print(f"\nâœ… All tests done. Comparison saved to: {csv_path}")
    
    # æ‰“å°æœ€ç»ˆæ§åˆ¶å°è¡¨æ ¼
    print("\n" + "ç»¼ åˆ å¯¹ æ¯” è¡¨".center(60, "-"))
    print(f"{'Model':<25} {'R-1':<8} {'R-2':<8} {'R-L':<8} {'AvgLen':<8}")
    for r in all_summary_results:
        print(f"{r['model_name']:<25} {r['rouge1']:<8.4f} {r['rouge2']:<8.4f} {r['rougeL']:<8.4f} {r['avg_len']:<8.1f}")

if __name__ == "__main__":
    main()