"""
ROUGEè¯„ä¼°è„šæœ¬
è¯„ä¼°æ¨¡å‹åœ¨TL;DRæ•°æ®é›†ä¸Šçš„æ‘˜è¦è´¨é‡
"""

import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

import torch
import json
import csv
from datetime import datetime
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_from_disk
import evaluate

# =========================================================
# é…ç½®
# =========================================================

MODELS_TO_EVALUATE = [
    {
        "name": "Base-Qwen3-1.7B",
        "path": "../models/Qwen3-1.7B",    
    },
    {
        "name": "SFT-Full",
        "path": "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint",
    },
    {
        "name": "grpo-baseline",
        "path": "/workspace/pj-RL/experiments3/qwen3-grpo-final-10k/checkpoint-1500",
    },
    {
        "name": "grpo-axis",
        "path": "/workspace/pj-RL/experiments3/qwen3-grpo-axis-merged/checkpoint-1200-merged",
    },
    {
        "name": "grpo-hybrid",
        "path": "/workspace/pj-RL/qwen3-grpo-hybrid-rm/checkpoint-1950",
    },    

    # {
    #     "name": "PPO-Improved",
    #     "path": "../experiments3/qwen3-ppo-improved/final_checkpoint",
    # },
]

DATASET_PATH = "/workspace/pj-RL/datasets/openai_summarize_tldr"
TEST_SAMPLES = 500

GENERATION_CONFIG = {
    "max_new_tokens": 60,
    "do_sample": True,
    "top_p": 0.9,
    "temperature": 0.8,
    "repetition_penalty": 1.1,
}

RESULTS_DIR = "./rouge_results"
os.makedirs(RESULTS_DIR, exist_ok=True)

# =========================================================
# å‡½æ•°å®šä¹‰
# =========================================================

# ç¯å¢ƒå¯¹é½
def extract_post_only(prompt: str) -> str:
    """æå–POSTå†…å®¹ï¼Œä¸SFTè®­ç»ƒæ—¶ä¿æŒä¸€è‡´"""
    if "POST:" in prompt:
        prompt = prompt.split("POST:", 1)[1]
    
    if "TL;DR:" in prompt:
        post, _ = prompt.split("TL;DR:", 1)
        prompt = post.strip() + "\n\nTL;DR:"
    else:
        prompt = prompt.strip() + "\n\nTL;DR:"
    
    return prompt

def evaluate_model_rouge(model, tokenizer, test_dataset, device):
    # ã€ç¬¬ä¸€æ­¥ï¼šæŠ¢å…ˆåˆå§‹åŒ– ROUGEã€‘
    # åªè¦è¿™è¡Œè·‘é€šäº†ï¼Œå°±è¯´æ˜ç½‘ç»œå’Œç¯å¢ƒæ²¡é—®é¢˜ï¼Œåé¢å¯ä»¥æ”¾å¿ƒæ¨ç†
    print("ğŸ“‹ æ­£åœ¨åˆå§‹åŒ– ROUGE è¯„ä¼°å™¨ (ä½¿ç”¨é•œåƒæº)...")
    try:
        rouge = evaluate.load("rouge")
        print("âœ… ROUGE è¯„ä¼°å™¨åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ ROUGE åŠ è½½å¤±è´¥ï¼æŠ¥é”™ä¿¡æ¯: {e}")
        print("è¯·æ£€æŸ¥æ˜¯å¦æ‰§è¡Œäº† export HF_ENDPOINT='https://hf-mirror.com'")
        raise e  # ç›´æ¥ç»ˆæ­¢ï¼Œä¸å†è·‘åé¢çš„æ¨ç†

    """è¯„ä¼°æ¨¡å‹çš„ROUGEåˆ†æ•°"""
    model.eval()
    predictions = []
    references = []
    
    print(f"Generating summaries for {len(test_dataset)} samples...")
    
    for example in tqdm(test_dataset):
        raw_prompt = example["prompt"]
        reference_summary = example["label"]
        clean_prompt = extract_post_only(raw_prompt)
        
        inputs = tokenizer(
            clean_prompt, 
            return_tensors="pt",
            truncation=True,
            max_length=1024
        ).to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **GENERATION_CONFIG,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "TL;DR:" in full_text:
            predicted_summary = full_text.split("TL;DR:")[-1].strip()
        else:
            predicted_summary = full_text.strip()
        
        predictions.append(predicted_summary)
        references.append(reference_summary)
    
    print("Computing ROUGE scores...")
    # rouge = evaluate.load("rouge")
    rouge_scores = rouge.compute(
        predictions=predictions, 
        references=references,
        use_stemmer=True
    )
    
    return rouge_scores, predictions, references

# =========================================================
# ä¸»å‡½æ•°
# =========================================================

def main():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    print(f"Loading dataset: {DATASET_PATH}")
    dataset = load_from_disk(DATASET_PATH)
    test_dataset = dataset["test"].select(range(TEST_SAMPLES))
    print(f"Loaded {len(test_dataset)} test samples")
    
    all_results = []
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    for model_config in MODELS_TO_EVALUATE:
        model_name = model_config["name"]
        model_path = model_config["path"]
        
        print(f"\nEvaluating: {model_name}")
        print(f"Path: {model_path}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            device_map="auto",
        )
        
        rouge_scores, predictions, references = evaluate_model_rouge(
            model, tokenizer, test_dataset, device
        )
        
        print(f"\n{model_name} ROUGE Scores:")
        print(f"  ROUGE-1: {rouge_scores['rouge1']:.4f}")
        print(f"  ROUGE-2: {rouge_scores['rouge2']:.4f}")
        print(f"  ROUGE-L: {rouge_scores['rougeL']:.4f}")
        print(f"  ROUGE-Lsum: {rouge_scores['rougeLsum']:.4f}")
        
        result_detail = {
            "model_name": model_name,
            "model_path": model_path,
            "timestamp": timestamp,
            "test_samples": TEST_SAMPLES,
            "rouge_scores": {
                "rouge1": float(rouge_scores['rouge1']),
                "rouge2": float(rouge_scores['rouge2']),
                "rougeL": float(rouge_scores['rougeL']),
                "rougeLsum": float(rouge_scores['rougeLsum']),
            },
            "generation_config": GENERATION_CONFIG,
        }
        
        json_path = f"{RESULTS_DIR}/{model_name}_{timestamp}.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(result_detail, f, indent=2, ensure_ascii=False)
        print(f"Results saved to: {json_path}")
        
        all_results.append({
            "model_name": model_name,
            "timestamp": timestamp,
            "rouge1": float(rouge_scores['rouge1']),
            "rouge2": float(rouge_scores['rouge2']),
            "rougeL": float(rouge_scores['rougeL']),
            "rougeLsum": float(rouge_scores['rougeLsum']),
        })
        
        del model
        del tokenizer
        torch.cuda.empty_cache()
    
    csv_path = f"{RESULTS_DIR}/rouge_comparison_{timestamp}.csv"
    with open(csv_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f, 
            fieldnames=["model_name", "timestamp", "rouge1", "rouge2", "rougeL", "rougeLsum"]
        )
        writer.writeheader()
        writer.writerows(all_results)
    
    print(f"\nComparison saved to: {csv_path}")
    print("\nROUGE Comparison:")
    print(f"{'Model':<30} {'ROUGE-1':<10} {'ROUGE-2':<10} {'ROUGE-L':<10}")
    print("-" * 60)
    for result in all_results:
        print(f"{result['model_name']:<30} {result['rouge1']:<10.4f} {result['rouge2']:<10.4f} {result['rougeL']:<10.4f}")

if __name__ == "__main__":
    main()
