import os
import torch
from tqdm import tqdm
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from peft import LoraConfig

# ===============================
# 0. åŸºç¡€é…ç½®
# ===============================
device = "cuda"
torch_dtype = torch.float16
sft_model_path = "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint"
rm_model_path  = "/workspace/pj-RL/experiments3/qwen3-rm/final_rm"
output_dir = "/workspace/pj-RL/experiments3/qwen3-ppo-final"

os.makedirs(output_dir, exist_ok=True)

# ===============================
# 1. Tokenizer (ä¿®å¤ Mistral Regex)
# ===============================
tokenizer = AutoTokenizer.from_pretrained(
    sft_model_path,
    trust_remote_code=True,
    fix_mistral_regex=True,
)
# PPO è®­ç»ƒå»ºè®®å·¦ä¾§å¡«å……ï¼Œä»¥ä¾¿ generate æ­£å¸¸å·¥ä½œ
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" 

# ===============================
# 2. PPO Config (é’ˆå¯¹ TRL 0.9.6 ä¼˜åŒ–)
# ===============================
config = PPOConfig(
    learning_rate=1.41e-5,
    batch_size=32,               # æ¯ 32 æ¡æ•°æ®æ‰§è¡Œä¸€æ¬¡ PPO æ›´æ–°
    mini_batch_size=2,           # 24GB æ˜¾å­˜å•å¡å»ºè®®è®¾ä¸º 2ï¼Œé˜²æ­¢ OOM
    gradient_accumulation_steps=16, 
    ppo_epochs=4,                # æ¯ä¸€æ‰¹æ•°æ®é‡å¤ä¼˜åŒ–çš„æ¬¡æ•°
    target_kl=0.1,               # é™åˆ¶æ¨¡å‹ä¸ SFT æ¨¡å‹çš„åå·®
    init_kl_coef=0.2,
    optimize_cuda_cache=True,    # 0.9.6 ç‰¹æœ‰ï¼šæ¯æ­¥æ¸…ç†æ˜¾å­˜ç¢ç‰‡
    seed=42,
)

# ===============================
# 3. åŠ è½½æ¨¡å‹ (Policy + Reference)
# ===============================
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# è‡ªåŠ¨åˆ›å»ºå¸¦ Value Head çš„ Causal LM
policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    sft_model_path,
    peft_config=lora_config,
    trust_remote_code=True,
    torch_dtype=torch_dtype,
    device_map="auto",
)

# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (24GB æ˜¾å­˜å¿…é¡»å¼€å¯)
policy_model.gradient_checkpointing_enable()

# åˆ›å»ºå†»ç»“çš„å‚è€ƒæ¨¡å‹
ref_model = create_reference_model(policy_model)

# ===============================
# 4. åŠ è½½å¥–åŠ±æ¨¡å‹ (Reward Model)
# ===============================
reward_model = AutoModelForSequenceClassification.from_pretrained(
    rm_model_path,
    trust_remote_code=True,
    torch_dtype=torch_dtype,
    device_map="auto",
)
reward_model.eval()

# ===============================
# 5. æ•°æ®é›†å‡†å¤‡
# ===============================
raw_dataset = load_from_disk("/workspace/pj-RL/datasets/summarize_from_feedback")["train"]

def tokenize_fn(example):
    prompt = f"POST: {example['info']['post']}\n\nTL;DR:"
    # æ³¨æ„ï¼šè¿™é‡Œåªå¤„ç† input_idsï¼Œä¸è¿›è¡Œ padding
    inputs = tokenizer(prompt, truncation=True, max_length=512)
    return {
        "input_ids": inputs["input_ids"],
        "query": prompt
    }

# é€‰å– 500 æ¡è¿›è¡Œ Baseline è®­ç»ƒ
ppo_dataset = raw_dataset.select(range(500)).map(tokenize_fn, remove_columns=raw_dataset.column_names)
ppo_dataset.set_format(type="torch")

def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}

# ===============================
# 6. åˆå§‹åŒ– PPOTrainer
# ===============================
ppo_trainer = PPOTrainer(
    config=config,
    model=policy_model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=ppo_dataset,
    data_collator=collator,
)
l l
# ===============================
# 7. è®­ç»ƒå¾ªç¯
# ===============================
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "max_new_tokens": 128,  # å¢åŠ é•¿åº¦ï¼Œè§£å†³æ‘˜è¦å†™ä¸å®Œçš„é—®é¢˜
}

print("\nğŸš€ Starting PPO Training Baseline...\n")

for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # --- Step 1: Rollout (æ¨¡å‹ç”Ÿæˆ) ---
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

    # --- Step 2: Scoring (RM æ‰“åˆ†) ---
    # å¥–åŠ±æ¨¡å‹é€šå¸¸é¢„æœŸæ ¼å¼ä¸º: Prompt + Response
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    
    with torch.no_grad():
        # å‡è®¾ RM è¾“å‡ºçš„ logits çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯å¥–åŠ±åˆ†æ•°
        outputs = reward_model(**inputs)
        # è·å–åˆ†æ•°å¹¶è½¬ä¸º tensor list
        rewards = [torch.tensor(score.item()) for score in outputs.logits]

    # --- Step 3: PPO Step (æ›´æ–°æ¨¡å‹) ---
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    
    # æ‰“å°ç›‘æ§æŒ‡æ ‡
    ppo_trainer.log_stats(stats, batch, rewards)

    # --- Step 4: ä¿å­˜ Checkpoint ---
    if (epoch + 1) % 50 == 0:
        ppo_trainer.save_pretrained(os.path.join(output_dir, f"step_{epoch+1}"))

# æœ€ç»ˆä¿å­˜
ppo_trainer.save_pretrained(os.path.join(output_dir, "final_ppo_model"))
print(f"\nâœ… Training finished. Model saved to {output_dir}")