# import os
# import torch
# from datasets import load_from_disk
# from transformers import AutoTokenizer, AutoModelForSequenceClassification
# from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
# from trl.experimental.ppo.ppo_trainer import PolicyAndValueWrapper
# from peft import LoraConfig
# from types import SimpleNamespace

# # ===============================
# # 0. åŸºç¡€é…ç½®
# # ===============================
# device = "cuda"
# torch_dtype = torch.float16


# # patch PolicyAndValueWrapper to expose gradient checkpoint toggles (needed by unwrap_model_for_generation)
# def _gc_disable(self):
#     if hasattr(self.policy, "gradient_checkpointing_disable"):
#         self.policy.gradient_checkpointing_disable()
#     if hasattr(self.value_model, "gradient_checkpointing_disable"):
#         self.value_model.gradient_checkpointing_disable()


# def _gc_enable(self):
#     if hasattr(self.policy, "gradient_checkpointing_enable"):
#         self.policy.gradient_checkpointing_enable()
#     if hasattr(self.value_model, "gradient_checkpointing_enable"):
#         self.value_model.gradient_checkpointing_enable()


# PolicyAndValueWrapper.gradient_checkpointing_disable = _gc_disable
# PolicyAndValueWrapper.gradient_checkpointing_enable = _gc_enable

# # monkey patch forward to always return an object with logits (and optional past_key_values)
# def _pv_forward(self, **kwargs):
#     policy_out = self.policy(**kwargs)
#     # policy_out may be tuple from ValueHead wrapper; extract logits and pkv
#     if isinstance(policy_out, tuple):
#         logits = policy_out[0]
#         pkv = policy_out[3] if len(policy_out) > 3 else None
#         policy_ns = SimpleNamespace(logits=logits, past_key_values=pkv)
#     else:
#         policy_ns = policy_out
#     # value model output already returned separately
#     output = self.critic_backbone(**kwargs)
#     logits_v = self.value_model.score(output.hidden_states[-1])
#     return policy_ns, logits_v


# PolicyAndValueWrapper.forward = _pv_forward

# sft_model_path = "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint"
# rm_model_path  = "/workspace/pj-RL/experiments3/qwen3-rm/final_rm"

# # ===============================
# # 1. Tokenizerï¼ˆä¿® regexï¼‰
# # ===============================
# tokenizer = AutoTokenizer.from_pretrained(
#     sft_model_path,
#     trust_remote_code=True,
#     fix_mistral_regex=True,
# )
# tokenizer.pad_token = tokenizer.eos_token

# # ===============================
# # 2. Policy Model + LoRA
# # ===============================
# lora_config = LoraConfig(
#     r=8,
#     lora_alpha=16,
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM",
# )

# policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(
#     sft_model_path,
#     peft_config=lora_config,
#     trust_remote_code=True,
#     dtype=torch_dtype,
#     device_map="auto",
# )
# policy_model.config.use_cache = False

# # ---------- ã€ä¿®æ”¹ 1ã€‘ç¡®ä¿ç”Ÿæˆconfigå­˜åœ¨ ----------
# if not hasattr(policy_model, "generation_config"):
#     policy_model.generation_config = policy_model.pretrained_model.generation_config

# # ---------- ã€ä¿®æ”¹ 2ã€‘å¼ºåˆ¶æ¨¡å‹è¿”å› ModelOutput è€Œä¸æ˜¯ tupleï¼Œå¹¶è¾“å‡º hidden_states ----------
# policy_model.config.return_dict = True
# policy_model.config.output_hidden_states = True
# policy_model.pretrained_model.config.return_dict = True
# policy_model.pretrained_model.config.output_hidden_states = True
# # ensure v_head uses last hidden_state
# policy_model.config.output_attentions = False
# policy_model.pretrained_model.config.output_attentions = False

# # enable gradient checkpointing to save memory
# if hasattr(policy_model.pretrained_model, "gradient_checkpointing_enable"):
#     policy_model.pretrained_model.gradient_checkpointing_enable()

# # expose is_gradient_checkpointing flag if missing
# if not hasattr(policy_model, "is_gradient_checkpointing"):
#     policy_model.is_gradient_checkpointing = getattr(policy_model.pretrained_model, "is_gradient_checkpointing", False)

# # ===============================
# # 3. Reference Modelï¼ˆå†»ç»“ï¼‰
# # ===============================
# ref_model = create_reference_model(policy_model.pretrained_model)
# ref_model.eval()
# for p in ref_model.parameters():
#     p.requires_grad = False

# # ===============================
# # 4. Reward Modelï¼ˆå†»ç»“ï¼‰
# # ===============================
# reward_model = AutoModelForSequenceClassification.from_pretrained(
#     rm_model_path,
#     trust_remote_code=True,
#     dtype=torch_dtype,
#     device_map="auto",
# )
# reward_model.eval()
# for p in reward_model.parameters():
#     p.requires_grad = False

# # ===============================
# # 5. Value model wrapperï¼ˆPPO éœ€è¦ score æ¥å£ï¼‰
# # ===============================
# class ValueModelWrapper(torch.nn.Module):
#     def __init__(self, policy):
#         super().__init__()
#         self.policy = policy
#         self.v_head = policy.v_head
#         self.base_model_prefix = policy.pretrained_model.base_model_prefix
#         setattr(self, self.base_model_prefix, policy.pretrained_model)

#     def score(self, hidden_states):
#         return self.v_head(hidden_states).squeeze(-1)


# value_model = ValueModelWrapper(policy_model)

# # ===============================
# # 5. PPO Configï¼ˆ0.26.2ï¼‰
# # ===============================
# ppo_config = PPOConfig(
#     batch_size=1,
#     mini_batch_size=1,
#     gradient_accumulation_steps=1,
#     per_device_train_batch_size=1,
#     per_device_eval_batch_size=1,
#     learning_rate=1e-5,
#     num_ppo_epochs=1,
#     response_length=32,
# )

# # ===============================
# # 6. Datasetï¼ˆtokenized ä¾› PPOTrainer ä½¿ç”¨ï¼‰
# # ===============================
# raw_dataset = load_from_disk("/workspace/pj-RL/datasets/summarize_from_feedback")["train"]


# def preprocess(example):
#     prompt = f"{example['info']['post']}\n\nTL;DR:"
#     toks = tokenizer(
#         prompt,
#         truncation=True,
#         max_length=128,
#         return_attention_mask=True,
#     )
#     return {"input_ids": toks["input_ids"], "attention_mask": toks["attention_mask"]}


# ppo_train_dataset = raw_dataset.map(preprocess, remove_columns=raw_dataset.column_names)
# ppo_train_dataset = ppo_train_dataset.select(range(50))
# ppo_train_dataset.set_format(type="torch")
# ppo_eval_dataset = ppo_train_dataset.select(range(5))

# # ===============================
# # 7. PPOTrainerï¼ˆéœ€æä¾› reward_model / dataset / value_modelï¼‰
# # ===============================
# ppo_trainer = PPOTrainer(
#     ppo_config,
#     tokenizer,
#     policy_model,
#     ref_model,
#     reward_model,
#     ppo_train_dataset,
#     value_model,
#     eval_dataset=ppo_eval_dataset,
# )

# # ===============================
# # 8. ç›´æ¥ä½¿ç”¨ PPOTrainer å†…éƒ¨ train
# # ===============================
# print("\nğŸš€ Starting PPO training...\n")
# ppo_trainer.train()
# print("\nâœ… Finished PPO training.")




import os
import torch
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from trl.experimental.ppo.ppo_trainer import PolicyAndValueWrapper
from peft import LoraConfig
from types import SimpleNamespace

# ===============================
# 0. åŸºç¡€é…ç½®
# ===============================
device = "cuda"
torch_dtype = torch.float16

# patch PolicyAndValueWrapper to expose gradient checkpoint toggles
def _gc_disable(self):
    if hasattr(self.policy, "gradient_checkpointing_disable"):
        self.policy.gradient_checkpointing_disable()
    if hasattr(self.value_model, "gradient_checkpointing_disable"):
        self.value_model.gradient_checkpointing_disable()

def _gc_enable(self):
    if hasattr(self.policy, "gradient_checkpointing_enable"):
        self.policy.gradient_checkpointing_enable()
    if hasattr(self.value_model, "gradient_checkpointing_enable"):
        self.value_model.gradient_checkpointing_enable()

PolicyAndValueWrapper.gradient_checkpointing_disable = _gc_disable
PolicyAndValueWrapper.gradient_checkpointing_enable = _gc_enable

# monkey patch forward to always return an object with logits (and optional past_key_values)
def _pv_forward(self, **kwargs):
    policy_out = self.policy(**kwargs)
    if isinstance(policy_out, tuple):
        logits = policy_out[0]
        pkv = policy_out[3] if len(policy_out) > 3 else None
        policy_ns = SimpleNamespace(logits=logits, past_key_values=pkv)
    else:
        policy_ns = policy_out
    output = self.critic_backbone(**kwargs)
    logits_v = self.value_model.score(output.hidden_states[-1])
    return policy_ns, logits_v

PolicyAndValueWrapper.forward = _pv_forward

sft_model_path = "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint"
rm_model_path  = "/workspace/pj-RL/experiments3/qwen3-rm/final_rm"

# ===============================
# 1. Tokenizerï¼ˆä¿® regexï¼‰
# ===============================
tokenizer = AutoTokenizer.from_pretrained(
    sft_model_path,
    trust_remote_code=True,
    fix_mistral_regex=True,
)
tokenizer.pad_token = tokenizer.eos_token

# ===============================
# 2. Policy Model + LoRA
# ===============================
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    sft_model_path,
    peft_config=lora_config,
    trust_remote_code=True,
    dtype=torch_dtype,
    device_map="auto",
)
policy_model.config.use_cache = False

# ---------- ç¡®ä¿ç”Ÿæˆconfigå­˜åœ¨ ----------
if not hasattr(policy_model, "generation_config"):
    policy_model.generation_config = policy_model.pretrained_model.generation_config

# ---------- å¼ºåˆ¶è¾“å‡º dict + hidden_states ----------
policy_model.config.return_dict = True
policy_model.config.output_hidden_states = True
policy_model.pretrained_model.config.return_dict = True
policy_model.pretrained_model.config.output_hidden_states = True
policy_model.config.output_attentions = False
policy_model.pretrained_model.config.output_attentions = False

if hasattr(policy_model.pretrained_model, "gradient_checkpointing_enable"):
    policy_model.pretrained_model.gradient_checkpointing_enable()

if not hasattr(policy_model, "is_gradient_checkpointing"):
    policy_model.is_gradient_checkpointing = getattr(policy_model.pretrained_model, "is_gradient_checkpointing", False)

# ===============================
# 3. Reference Modelï¼ˆå†»ç»“ï¼‰
# ===============================
ref_model = create_reference_model(policy_model.pretrained_model)
ref_model.eval()
for p in ref_model.parameters():
    p.requires_grad = False

# ===============================
# 4. Reward Modelï¼ˆå†»ç»“ï¼‰
# ===============================
reward_model = AutoModelForSequenceClassification.from_pretrained(
    rm_model_path,
    trust_remote_code=True,
    dtype=torch_dtype,
    device_map="auto",
)
reward_model.eval()
for p in reward_model.parameters():
    p.requires_grad = False

# ===============================
# 5. Value model wrapper
# ===============================
class ValueModelWrapper(torch.nn.Module):
    def __init__(self, policy):
        super().__init__()
        self.policy = policy
        self.v_head = policy.v_head
        self.base_model_prefix = policy.pretrained_model.base_model_prefix
        setattr(self, self.base_model_prefix, policy.pretrained_model)

    def score(self, hidden_states):
        return self.v_head(hidden_states).squeeze(-1)

value_model = ValueModelWrapper(policy_model)

# ===============================
# 5. PPO Config
# ===============================
ppo_config = PPOConfig(
    batch_size=1,
    mini_batch_size=1,
    gradient_accumulation_steps=1,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    learning_rate=1e-5,
    num_ppo_epochs=1,
    response_length=32,
)

# ===============================
# 6. Datasetï¼ˆåªç”¨ 5 æ¡è¿›è¡Œæµ‹è¯•ï¼‰
# ===============================
raw_dataset = load_from_disk("/workspace/pj-RL/datasets/summarize_from_feedback")["train"]

def preprocess(example):
    prompt = f"{example['info']['post']}\n\nTL;DR:"
    toks = tokenizer(
        prompt,
        truncation=True,
        max_length=128,
        return_attention_mask=True,
    )
    return {"input_ids": toks["input_ids"], "attention_mask": toks["attention_mask"], "prompt": prompt}

# åªå–å‰5æ¡æ•°æ®
test_dataset = raw_dataset.map(preprocess, remove_columns=raw_dataset.column_names)
test_dataset = test_dataset.select(range(5))
test_dataset.set_format(type="torch")

# ===============================
# 7. ä½¿ç”¨ policy + RM æµ‹è¯•ç”Ÿæˆå†…å®¹å’Œå¥–åŠ±
# ===============================
print("\nğŸš€ Testing generation and RM scoring for 5 examples...\n")

for i, batch in enumerate(test_dataset):
    input_ids = batch["input_ids"].unsqueeze(0).to(device)
    attention_mask = batch["attention_mask"].unsqueeze(0).to(device)
    prompt_text = batch["prompt"]

    # ç”Ÿæˆæ–‡æœ¬
    with torch.no_grad():
        outputs = policy_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_new_tokens=64,
        )
    gen_text = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)

    # å¥–åŠ±æ¨¡å‹æ‰“åˆ†
    with torch.no_grad():
        rm_inputs = tokenizer(gen_text, return_tensors="pt").to(device)
        reward_score = reward_model(**rm_inputs).logits.squeeze().item()

    print(f"Example {i+1}")
    print(f"Prompt: {prompt_text}")
    print(f"Generated: {gen_text}")
    print(f"Reward score: {reward_score}")
    print("-" * 80)

print("\nâœ… Finished testing.")
