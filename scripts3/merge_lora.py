import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# é…ç½®è·¯å¾„ - è¯·ç¡®ä¿è¿™äº›è·¯å¾„ä¸ä½ æœåŠ¡å™¨ä¸Šçš„å®é™…è·¯å¾„ä¸€è‡´
base_model_path = "/workspace/pj-RL/models/Qwen3-1.7B"
lora_adapter_path = "/workspace/pj-RL/experiments3/qwen3-sft-lora/final_checkpoint"
output_path = "/workspace/pj-RL/experiments3/qwen3-sft-lora-merged"

def merge():
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹...")
    # ä½¿ç”¨ bfloat16 èŠ‚çœå†…å­˜å¹¶ä¿æŒç²¾åº¦
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        device_map="cpu",  # åˆå¹¶å»ºè®®åœ¨ CPU ä¸Šè¿›è¡Œï¼Œé¿å…å ç”¨è®­ç»ƒç”¨çš„æ˜¾å­˜
        trust_remote_code=True
    )

    print(f"âš™ï¸ æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨...")
    model = PeftModel.from_pretrained(base_model, lora_adapter_path)

    print(f"ğŸ—ï¸ æ­£åœ¨åˆå¹¶æƒé‡ (Merge and Unload)...")
    # è¿™ä¸€æ­¥æ˜¯å°† LoRA å‚æ•°åŠ å›åˆ°åŸºåº§å‚æ•°ä¸­
    model = model.merge_and_unload()

    print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜å®Œæ•´æ¨¡å‹è‡³: {output_path}")
    model.save_pretrained(output_path)
    
    # ä¿å­˜åˆ†è¯å™¨ï¼ŒROUGE è„šæœ¬éœ€è¦å®ƒ
    tokenizer = AutoTokenizer.from_pretrained(lora_adapter_path, trust_remote_code=True)
    tokenizer.save_pretrained(output_path)

    print("âœ… åˆå¹¶æˆåŠŸï¼")

if __name__ == "__main__":
    merge()