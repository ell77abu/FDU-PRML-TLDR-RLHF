import torch
import json
import os
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from datasets import load_from_disk
from tqdm import tqdm

# ============================
# Config
# ============================
RM_PATH   = "/workspace/pj-RL/experiments3/qwen3-rm/final_rm"
DATA_PATH = "/workspace/pj-RL/datasets/summarize_from_feedback"
SAVE_PATH = "/workspace/pj-RL/experiments3/qwen3-rm-normalized"

SAMPLE_SIZE = 5000 
BATCH_SIZE  = 8
MAX_LENGTH  = 1024

os.makedirs(SAVE_PATH, exist_ok=True)

# ============================
# Load RM
# ============================
print("Loading Reward Model...")
tokenizer = AutoTokenizer.from_pretrained(RM_PATH, trust_remote_code=True)
model = AutoModelForSequenceClassification.from_pretrained(
    RM_PATH,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto"
)
model.eval()

# ============================
# Compute mean reward (ä½¿ç”¨éšæœºé‡‡æ ·)
# ============================
print("ğŸ² Shuffling dataset for representative sampling...")
dataset = load_from_disk(DATA_PATH)["train"].shuffle(seed=42)

n = min(SAMPLE_SIZE, len(dataset))
texts = []
for i in range(n):
    ex = dataset[i]
    prompt = f"{ex['info']['post']}\n\nTL;DR:"
    texts.append(prompt + ex["summaries"][ex["choice"]]["text"] + tokenizer.eos_token)

print(f"Computing mean RM score on {n} samples...")
all_scores = []
with torch.no_grad():
    for i in tqdm(range(0, n, BATCH_SIZE)):
        batch = texts[i : i + BATCH_SIZE]
        inputs = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt").to(model.device)
        scores = model(**inputs).logits.squeeze(-1)
        all_scores.extend(scores.cpu().float().tolist())

mean_reward = sum(all_scores) / len(all_scores)
print(f"  â†’ Human reference mean = {mean_reward:.6f}")

# ============================
# Apply calibration & Fix Architecture
# ============================
print("ğŸ”§ Applying zero-point calibration...")

# å¼ºåˆ¶æ›¿æ¢ä¸ºå¸¦ bias çš„å±‚
old_score = model.score
new_score = torch.nn.Linear(old_score.in_features, old_score.out_features, bias=True)
new_score.weight.data = old_score.weight.data
new_score.bias.data.fill_(-mean_reward) # æ ¸å¿ƒï¼šå°†å‡å€¼çš„è´Ÿå€¼å¡«å…¥ bias
model.score = new_score.to(model.device).to(model.dtype)

# ============================
# éªŒè¯ï¼šå†æ¬¡è¿è¡ŒåŒæ ·çš„ n æ¡æ•°æ®
# ============================
print("ğŸ” Verifying calibration on all samples...")
with torch.no_grad():
    verify_scores = []
    for i in range(0, n, BATCH_SIZE):
        batch = texts[i : i + BATCH_SIZE]
        inputs = tokenizer(batch, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt").to(model.device)
        scores = model(**inputs).logits.squeeze(-1)
        verify_scores.extend(scores.cpu().float().tolist())

new_mean = sum(verify_scores) / n
print(f"  â†’ New human reference mean = {new_mean:.6f} (Should be near 0)")

# ============================
# ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨ PyTorch æ ¼å¼ç¡®ä¿ bias è¢«ä¿å­˜ï¼‰
# ============================
print("Saving calibrated RM...")

# ä½¿ç”¨ PyTorch æ ¼å¼ä¿å­˜ï¼ˆ.binï¼‰è€Œä¸æ˜¯ safetensors
# è¿™æ ·å¯ä»¥ç¡®ä¿ bias å‚æ•°è¢«æ­£ç¡®ä¿å­˜
model.save_pretrained(SAVE_PATH, safe_serialization=False)
tokenizer.save_pretrained(SAVE_PATH)

# ä¿®æ”¹ config.json æ·»åŠ è‡ªå®šä¹‰å­—æ®µæ ‡è®°è¿™æ˜¯å½’ä¸€åŒ–æ¨¡å‹
config_file = os.path.join(SAVE_PATH, "config.json")
with open(config_file, "r") as f:
    config = json.load(f)

# æ·»åŠ è‡ªå®šä¹‰å­—æ®µ
config["_normalized_rm"] = True
config["_normalization_bias"] = float(-mean_reward)
config["_score_head_has_bias"] = True

with open(config_file, "w") as f:
    json.dump(config, f, indent=2)

print(f"Done. Calibrated RM saved to: {SAVE_PATH}")
print(f"Config updated with normalization metadata")
print(f"   - Bias value: {-mean_reward:.6f}")
print(f"   - Saved in PyTorch format (.bin) to preserve bias layer")