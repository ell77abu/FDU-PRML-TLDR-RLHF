import os
import time
import torch
import wandb
from tqdm import tqdm
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from peft import LoraConfig

# ===============================
# 0. åŸºç¡€é…ç½®
# ===============================
device = "cuda"
torch_dtype = torch.float16
sft_model_path = "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint"
rm_model_path  = "/workspace/pj-RL/experiments3/qwen3-rm-normalized" 
output_dir = "/workspace/pj-RL/experiments3/qwen3-ppo-valuehead" 

os.makedirs(output_dir, exist_ok=True)

# ===============================
# 1. Tokenizer 
# ===============================
tokenizer = AutoTokenizer.from_pretrained(sft_model_path, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" 

# ===============================
# 2. PPO Config (æŒ‰ç…§ OpenAI è®ºæ–‡å»ºè®®å¾®è°ƒ)
# ===============================
config = PPOConfig(
    learning_rate=3e-6,
    batch_size=64,               # å¢å¤§æœ‰æ•ˆ Batch ä»¥ç¨³å®šè®­ç»ƒ
    mini_batch_size=1,           # 24GB æ˜¾å­˜ä¿å‘½é…ç½®
    gradient_accumulation_steps=64,
    ppo_epochs=1,                
    target_kl=0.05,              # ç•¥å¾®æ”¾å®½ç›®æ ‡ï¼Œç»™æ¨¡å‹æ¢ç´¢ç©ºé—´
    init_kl_coef=0.04,          # é™ä½åˆå§‹KLç³»æ•°ã€‚ã€‚ã€‚ğŸ˜„
    adap_kl_ctrl=True,
    optimize_cuda_cache=True,    
    seed=42,
    log_with="wandb",
    whiten_rewards=True,        # TRL ç‰¹æ€§ï¼šå°†ä¸€ä¸ª Batch å†…çš„å¥–åŠ±å½’ä¸€åŒ–åˆ°å‡å€¼ 0ï¼Œæ ‡å‡†å·® 1
)

# ===============================
# 3. æ¨¡å‹åŠ è½½ï¼šç­–ç•¥éš”ç¦»ä¸ Value Head åˆå§‹åŒ–
# ===============================

# é…ç½® LoRA å‚æ•°
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    modules_to_save=["v_head"], # ç¡®ä¿ Value Head ç‹¬ç«‹è®­ç»ƒ
)

# åŠ è½½ Policy æ¨¡å‹
policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    sft_model_path,
    peft_config=lora_config,
    torch_dtype=torch_dtype,
    device_map="auto",
)
policy_model.gradient_checkpointing_enable()
policy_model.train()

print("\nğŸ”¹ Initializing Value Function with Normalized RM weights...")

# 1. åŠ è½½åŸå§‹æƒé‡å­—å…¸
rm_state_dict = torch.load(os.path.join(rm_model_path, "pytorch_model.bin"), map_location="cpu")

# 2. ä» config.json ä¸­è¯»å–æ‰‹åŠ¨ä¿å­˜çš„å½’ä¸€åŒ–åç½® (_normalization_bias)
import json
with open(os.path.join(rm_model_path, "config.json"), "r") as f:
    rm_config = json.load(f)

# è·å–ä¿å­˜çš„ biasï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™é»˜è®¤ä¸º 0
norm_bias_shift = rm_config.get("_normalization_bias", 0.0)
print(f"  ğŸ” Found _normalization_bias in config: {norm_bias_shift}")

with torch.no_grad():
    # æ‹·è´æƒé‡ (Weight ä¸éœ€è¦å˜)
    if "score.weight" in rm_state_dict:
        policy_model.v_head.summary.weight.copy_(rm_state_dict["score.weight"])
        
        # æ‹·è´åç½® (Bias éœ€è¦åŠ ä¸Šå½’ä¸€åŒ–åç§»é‡)
        # æ³¨æ„ï¼šå¦‚æœåŸå§‹ RM æœ‰ biasï¼Œæˆ‘ä»¬è¦åŠ ä¸Šåç§»ï¼›å¦‚æœåŸå§‹ RM æ²¡ biasï¼Œå°±ç›´æ¥è®¾ä¸ºåç§»å€¼
        if "score.bias" in rm_state_dict:
            original_bias = rm_state_dict["score.bias"]
            # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šå½’ä¸€åŒ–åçš„ Bias = åŸå§‹ Bias + ä¿®æ­£å€¼
            policy_model.v_head.summary.bias.copy_(original_bias + norm_bias_shift)
            print(f"Value Head bias initialized: {original_bias.item():.6f} + ({norm_bias_shift:.6f})")
        else:
            # å¦‚æœåŸå§‹ RM æ²¡ bias (Linear å±‚ bias=False)ï¼ŒTRL çš„ v_head é»˜è®¤æ˜¯æœ‰ bias çš„
            policy_model.v_head.summary.bias.fill_(norm_bias_shift)
            print(f"Value Head bias initialized with shift: {norm_bias_shift:.6f}")
            
    else:
        print("Error: 'score.weight' not found in RM state_dict!")

# é‡Šæ”¾ä¸´æ—¶æ˜¾å­˜
del rm_state_dict
torch.cuda.empty_cache()

# åˆ›å»ºå†»ç»“çš„å‚è€ƒæ¨¡å‹
ref_model = create_reference_model(policy_model)
for param in ref_model.parameters():
    param.requires_grad = False

# åŠ è½½ç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ç”¨äºè¯„åˆ†
reward_model = AutoModelForSequenceClassification.from_pretrained(
    rm_model_path,
    torch_dtype=torch_dtype,
    device_map="cpu", 
)
reward_model.eval()
for param in reward_model.parameters():
    param.requires_grad = False

# ===============================
# 4. æ•°æ®é›†å‡†å¤‡ (ä¿æŒ 1024 é•¿åº¦)
# ===============================
raw_dataset = load_from_disk("/workspace/pj-RL/datasets/summarize_from_feedback")["train"]

def tokenize_fn(example):
    prompt = f"{example['info']['post']}\n\nTL;DR:"
    inputs = tokenizer(prompt, truncation=True, max_length=1024)
    return {"input_ids": inputs["input_ids"], "query": prompt}

ppo_dataset = raw_dataset.shuffle(seed=42).select(range(15000)).map(tokenize_fn, remove_columns=raw_dataset.column_names)
ppo_dataset.set_format(type="torch")

def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}

# ===============================
# 5. åˆå§‹åŒ– PPOTrainer
# ===============================
ppo_trainer = PPOTrainer(
    config=config,
    model=policy_model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=ppo_dataset,
    data_collator=collator,
)

# ===============================
# 6. è®­ç»ƒå¾ªç¯ä¸ WandB ç›‘æ§å¢å¼º
# ===============================
generation_kwargs = {
    "top_k": 0.0, "top_p": 0.95, "do_sample": True,
    "temperature": 0.7, "max_new_tokens": 60,
    "pad_token_id": tokenizer.pad_token_id,
}

wandb.init(project="qwen3-ppo-valuehead", name=f"Qwen3-PPO-ValueHead-{int(time.time())}")

for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # --- Step 1: Rollout ---
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

    # --- Step 2: Scoring ---
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    
    reward_model.to(device)
    with torch.no_grad():
        outputs = reward_model(**inputs)
        # æ ¹æ®ä½ çš„å®éªŒç»“è®ºï¼šè¿™é‡Œå¯ä»¥ä½¿ç”¨å½’ä¸€åŒ–åˆ†æ•°ï¼Œå¦‚å‘ç° KL ä¸‹é™å†è€ƒè™‘ * 2.0
        rewards = [score for score in outputs.logits.flatten()]
    reward_model.to("cpu")

    # --- Step 3: PPO Step ---
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

    # --- Step 4: è‡ªå®šä¹‰ WandB è®°å½• (å¢åŠ å¯¹ Value Function çš„ç›‘æ§) ---
    wandb.log({
        "ppo/val/vpred_mean": stats.get("ppo/val/vpred", 0).mean() if hasattr(stats.get("ppo/val/vpred"), 'mean') else 0,
        "ppo/val/error_mean": stats.get("ppo/val/error", 0),
        "reward/mean_batch": torch.stack(rewards).mean().item(),
        "generation/sample_text": wandb.Html(f"<b>Prompt:</b> {batch['query'][0]}<br><b>Response:</b> {batch['response'][0]}")
    })

    if (epoch + 1) % 200 == 0:
        ppo_trainer.save_pretrained(os.path.join(output_dir, f"step_{epoch+1}"))

ppo_trainer.save_pretrained(os.path.join(output_dir, "final_model"))
wandb.finish()