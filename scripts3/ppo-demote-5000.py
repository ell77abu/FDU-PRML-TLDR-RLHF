import os
import time
import torch
import wandb
from tqdm import tqdm
from datasets import load_from_disk
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from peft import LoraConfig

# ===============================
# 0. åŸºç¡€é…ç½®
# ===============================
device = "cuda"
torch_dtype = torch.float16
sft_model_path = "/workspace/pj-RL/experiments3/qwen3-sft/final_checkpoint"
rm_model_path  = "/workspace/pj-RL/experiments3/qwen3-rm-normalized" # ä½¿ç”¨å½’ä¸€åŒ–åçš„å¥–åŠ±æ¨¡å‹
output_dir = "/workspace/pj-RL/experiments3/qwen3-ppo-final"

os.makedirs(output_dir, exist_ok=True)

# ===============================
# 0.1 wandb é…ç½®
# ===============================
run = wandb.init(
    project="prml-norm-ppo-5000",
    name=f"Qwen3-PPO-demote-norm-5000-{int(time.time())}",
    config={
        "model": sft_model_path,
        "reward_model": rm_model_path,
        "train_samples": 5000,
        "learning_rate": 5e-6,
        "batch_size":32,
        "mini_batch_size": 2,
        "gradient_accumulation_steps": 16,
        "ppo_epochs": 1,
        "target_kl": 0.05,
        "init_kl_coef": 0.04,
        "max_new_tokens": 60,
        "rm_offload": "cpu",  # RM æ”¾åœ¨ CPUï¼Œæ¨ç†æ—¶ä¸´æ—¶ç§»åˆ° GPU
    },
)

# ===============================
# 1. Tokenizer (ä¿®å¤ Mistral Regex)
# ===============================
tokenizer = AutoTokenizer.from_pretrained(
    sft_model_path,
    trust_remote_code=True,
    fix_mistral_regex=True,
)
# PPO è®­ç»ƒå»ºè®®å·¦ä¾§å¡«å……ï¼Œä»¥ä¾¿ generate æ­£å¸¸å·¥ä½œ
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" 

# ===============================
# 2. PPO Config (é’ˆå¯¹ TRL 0.9.6 ä¼˜åŒ–)
# ===============================
config = PPOConfig(
    learning_rate=5e-6,
    batch_size=64,               # å‡å°åˆ° 16 é¿å… OOM
    mini_batch_size=2,           # å›é€†
    gradient_accumulation_steps=32,
    ppo_epochs=1,                # æ¯ä¸€æ‰¹æ•°æ®é‡å¤ä¼˜åŒ–çš„æ¬¡æ•°
    #B. ä¸ºä»€ä¹ˆå»ºè®® ppo_epochs=1ï¼Ÿåœ¨å°æ•°æ®é›†ï¼ˆ500æ¡ï¼‰æ—¶ï¼Œä¸ºäº†è®©æ¨¡å‹â€œåƒé€â€æ•°æ®ï¼Œæˆ‘ä»¬è®¾ä¸º 2ã€‚ä½†åœ¨ 5000 æ¡æ—¶ï¼Œæ•°æ®é‡è¶³å¤Ÿä¸°å¯Œï¼Œè®¾ä¸º 1 å¯ä»¥æ˜¾è‘—é™ä½ Reward Hacking çš„é£é™©ã€‚æ¨¡å‹æ¯æ­¥åªçœ‹ä¸€æ¬¡æ–°æ•°æ®ï¼ŒKL å¢é•¿ä¼šçº¿æ€§ä¸”å¹³ç¨³ï¼Œè€Œä¸æ˜¯æŒ‡æ•°çº§è·³å˜ã€‚
    target_kl=0.05,              # é™åˆ¶æ¨¡å‹ä¸ SFT æ¨¡å‹çš„åå·®
    init_kl_coef=0.1,           # KLæ•£åº¦æƒ©ç½šç³»æ•°
    # é’ˆå¯¹å¤§æ•°æ®é‡æ–°å¢ï¼šadap_kl_ctrl
    adap_kl_ctrl=True,
    optimize_cuda_cache=True,    # 0.9.6 ç‰¹æœ‰ï¼šæ¯æ­¥æ¸…ç†æ˜¾å­˜ç¢ç‰‡
    seed=42,
    # wandb é…ç½®
    log_with="wandb",
    # tracker_project_name="qwen3-ppo",
)

# ===============================
# 3. åŠ è½½æ¨¡å‹ (Policy + Reference)
# ===============================
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# è‡ªåŠ¨åˆ›å»ºå¸¦ Value Head çš„ Causal LM
policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(
    sft_model_path,
    peft_config=lora_config,
    trust_remote_code=True,
    torch_dtype=torch_dtype,
    device_map="auto",
)

# å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ (24GB æ˜¾å­˜å¿…é¡»å¼€å¯)
policy_model.gradient_checkpointing_enable()

# åˆ›å»ºå†»ç»“çš„å‚è€ƒæ¨¡å‹
ref_model = create_reference_model(policy_model)
for param in ref_model.parameters():
    param.requires_grad = False
# ===============================
# 4. åŠ è½½å¥–åŠ±æ¨¡å‹ (Reward Model)
# ===============================
print("\nğŸ”¹ Loading Reward Model (on CPU to save GPU memory)...")
reward_model = AutoModelForSequenceClassification.from_pretrained(
    rm_model_path,
    trust_remote_code=True,
    torch_dtype=torch_dtype,
    device_map="cpu",  # æ”¾åœ¨ CPUï¼Œæ¨ç†æ—¶ä¸´æ—¶ç§»åˆ° GPU
)

# ğŸ”§ å½’ä¸€åŒ–æ¨¡å‹ bias åŠ è½½é€‚é…
if hasattr(reward_model, "score") and reward_model.score.bias is None:
    print("  âš ï¸  Score head has no bias, loading from state dict...")
    state_dict_path = os.path.join(rm_model_path, "pytorch_model.bin")
    if os.path.exists(state_dict_path):
        state_dict = torch.load(state_dict_path, map_location="cpu")
        if "score.bias" in state_dict:
            old_score = reward_model.score
            new_score = torch.nn.Linear(old_score.in_features, old_score.out_features, bias=True)
            new_score.weight.data = old_score.weight.data
            new_score.bias.data = state_dict["score.bias"].to(dtype=torch_dtype)
            reward_model.score = new_score
            print(f"  âœ… Loaded normalized RM with bias = {new_score.bias.item():.6f}")
        else:
            print("  âš ï¸  WARNING: No bias found, RM may not be normalized!")
elif hasattr(reward_model, "score") and reward_model.score.bias is not None:
    print(f"  âœ… RM loaded with bias = {reward_model.score.bias.item():.6f}")

reward_model.eval()
for param in reward_model.parameters():
    param.requires_grad = False
print("  ğŸ’¡ RM will be moved to GPU only during inference")

# ===============================
# 5. æ•°æ®é›†å‡†å¤‡ 
# tokenizer.decode(output_ids[0, len(input_ids[0]):], skip_special_tokens=True)
# ===============================
raw_dataset = load_from_disk("/workspace/pj-RL/datasets/summarize_from_feedback")["train"]

def tokenize_fn(example):
    prompt = f"{example['info']['post']}\n\nTL;DR:" # POST: å¼€å¤´æ”¹ä¸ºæ­£å¸¸æ ¼å¼
    # æ³¨æ„ï¼šè¿™é‡Œåªå¤„ç† input_idsï¼Œä¸è¿›è¡Œ padding
    inputs = tokenizer(prompt, truncation=True, max_length=1024)
    return {
        "input_ids": inputs["input_ids"],
        "query": prompt
    }

# é€‰å– 500 æ¡è¿›è¡Œ Baseline è®­ç»ƒ
ppo_dataset = raw_dataset.shuffle(seed=42).select(range(5000)).map(tokenize_fn, remove_columns=raw_dataset.column_names)
ppo_dataset.set_format(type="torch")

def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}

# ===============================
# 6. åˆå§‹åŒ– PPOTrainer
# ===============================
ppo_trainer = PPOTrainer(
    config=config,
    model=policy_model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=ppo_dataset,
    data_collator=collator,
)
# ===============================
# 7. è®­ç»ƒå¾ªç¯
# ===============================
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 0.95,
    "do_sample": True,
    "temperature": 0.7,
    "pad_token_id": tokenizer.pad_token_id,
    "eos_token_id": tokenizer.eos_token_id,
    "max_new_tokens": 60,  # å¢åŠ é•¿åº¦ï¼Œè§£å†³æ‘˜è¦å†™ä¸å®Œçš„é—®é¢˜
}

print("\nğŸš€ Starting PPO Training Baseline...\n")

for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # --- Step 1: Rollout (æ¨¡å‹ç”Ÿæˆ) ---
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

    # --- Step 2: Scoring (RM æ‰“åˆ†) ---
    # å¥–åŠ±æ¨¡å‹é€šå¸¸é¢„æœŸæ ¼å¼ä¸º: Prompt + Response
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    
    # ä¸´æ—¶å°† RM ç§»åˆ° GPU è¿›è¡Œæ¨ç†
    reward_model.to(device)
    with torch.no_grad():
        # å‡è®¾ RM è¾“å‡ºçš„ logits çš„ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯å¥–åŠ±åˆ†æ•°
        outputs = reward_model(**inputs)
        # è·å–åˆ†æ•°å¹¶è½¬ä¸º tensor list
        # rewards = [torch.tensor(score.item()) for score in outputs.logits]
        # æ˜¾å¼åœ°æ”¾å¤§å¥–åŠ±ä¿¡å·ï¼Œæˆ–è€…è¿›è¡ŒåŠ¨æ€æ ‡å‡†åŒ–
        rewards = [torch.tensor(score.item() * 2.0) for score in outputs.logits] # å°è¯•æ”¾å¤§ 2 å€
    # æ¨ç†å®Œæˆåç§»å› CPU é‡Šæ”¾æ˜¾å­˜
    reward_model.to("cpu")
    torch.cuda.empty_cache()

    # --- Step 3: PPO Step (æ›´æ–°æ¨¡å‹) ---
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)

    # æ‰“å°ç›‘æ§æŒ‡æ ‡
    ppo_trainer.log_stats(stats, batch, rewards)

    # --- Step 4: wandb è®°å½• ---
    # TRL çš„ log_stats å·²è‡ªåŠ¨è®°å½•å¤§éƒ¨åˆ†æŒ‡æ ‡ï¼Œè¿™é‡Œåªè¡¥å……å…³é”®çš„è‡ªå®šä¹‰æŒ‡æ ‡
    reward_scores = [r.item() for r in rewards]
    
    # è®¡ç®—ç”Ÿæˆæ–‡æœ¬é•¿åº¦
    response_lengths = [len(r) for r in response_tensors]
    
    wandb.log({
        "epoch": epoch,
        # === æœ€é‡è¦ï¼šReward ç»Ÿè®¡ ===
        "reward/mean": sum(reward_scores) / len(reward_scores),
        "reward/max": max(reward_scores),
        "reward/min": min(reward_scores),
        "reward/std": torch.tensor(reward_scores).std().item(),
        
        # === ç”Ÿæˆè´¨é‡æŒ‡æ ‡ ===
        "generation/length_mean": sum(response_lengths) / len(response_lengths),
        "generation/length_max": max(response_lengths),
        "generation/sample": batch["response"][0] if batch["response"] else "",
        
        # === æ ¸å¿ƒ PPO æŒ‡æ ‡ï¼ˆä» stats æå–ï¼‰===
        "ppo/loss/total": stats.get("ppo/loss/total", 0),
        "ppo/loss/policy": stats.get("ppo/loss/policy", 0),
        "ppo/loss/value": stats.get("ppo/loss/value", 0),
        "ppo/policy/entropy": stats.get("ppo/policy/entropy", 0),  # å¤šæ ·æ€§æŒ‡æ ‡
        "ppo/policy/approxkl": stats.get("ppo/policy/approxkl", 0),  # å®é™… KL
        "ppo/policy/clipfrac": stats.get("ppo/policy/clipfrac", 0),  # clip æ¯”ä¾‹
        "ppo/returns/mean": stats.get("ppo/returns/mean", 0),
        "ppo/val/vpred": stats.get("ppo/val/vpred", 0),
        "ppo/val/error": stats.get("ppo/val/error", 0),
    })

    # --- Step 4: ä¿å­˜ Checkpoint ---
    if (epoch + 1) % 50 == 0:
        ppo_trainer.save_pretrained(os.path.join(output_dir, f"step_{epoch+1}"))

# æœ€ç»ˆä¿å­˜
ppo_trainer.save_pretrained(os.path.join(output_dir, "final_ppo_model"))
print(f"\nâœ… Training finished. Model saved to {output_dir}")

# ç»“æŸ wandb è®°å½•
wandb.finish()