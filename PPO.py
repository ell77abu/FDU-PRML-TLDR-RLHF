import torch
from tqdm import tqdm
from transformers import AutoTokenizer
from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model
from datasets import load_dataset

# --- 1. é…ç½® ---
model_id = "./sft-tldr/final_checkpoint" # SFT æ¨¡å‹è·¯å¾„
rm_model_id = "./rm-tldr/final_rm"       # è®­ç»ƒå¥½çš„ RM è·¯å¾„
device = "cuda" if torch.cuda.is_available() else "cpu"

config = PPOConfig(
    model_name="qwen-1.8b-ppo",
    learning_rate=1.41e-5,
    batch_size=32,          # æ¯æ¬¡ PPO æ›´æ–°ä½¿ç”¨çš„æ ·æœ¬æ€»æ•°
    mini_batch_size=4,      # æ˜¾å­˜é™åˆ¶ä¸‹çš„å¾®æ‰¹æ¬¡
    gradient_accumulation_steps=8,
    optimize_cuda_cache=True,
    early_stopping=False,
    target_kl=0.1,          # ç›®æ ‡ KL æ•£åº¦
    init_kl_coeff=0.1,      # åˆå§‹ KL æƒ©ç½šç³»æ•°
    adap_kl_ctrl=True,      # åŠ¨æ€è°ƒæ•´ KL
)

# --- 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
# æ³¨æ„ï¼šPPO ç”Ÿæˆé€šå¸¸ç”¨ left padding
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "left" 

# åŠ è½½å¸¦ Value Head çš„ç­–ç•¥æ¨¡å‹
model = AutoModelForCausalLMWithValueHead.from_pretrained(
    model_id, 
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).to(device)

# åˆ›å»ºå‚è€ƒæ¨¡å‹ï¼ˆå†»ç»“çš„ SFTï¼Œç”¨äºè®¡ç®— KLï¼‰
ref_model = create_reference_model(model)

# åŠ è½½å¥–åŠ±æ¨¡å‹ï¼ˆRMï¼‰
from transformers import AutoModelForSequenceClassification
reward_model = AutoModelForSequenceClassification.from_pretrained(
    rm_model_id, 
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).to(device).eval()

# --- 3. æ•°æ®å¤„ç† ---
dataset = load_dataset("openai/summarize_from_feedback", "comparisons", split="train")
dataset = dataset.shuffle(seed=42).select(range(5000)) # PPO è¿­ä»£é€šå¸¸ä¸éœ€è¦æµ·é‡æ•°æ®

def tokenize(sample):
    # ä¸¥æ ¼å¯¹é½è®­ç»ƒæ—¶çš„ Prompt æ ¼å¼
    prompt = f"Post: {sample['info']['post']}\nTL;DR:"
    sample["input_ids"] = tokenizer.encode(prompt, truncation=True, max_length=512)
    sample["query"] = prompt
    return sample

dataset = dataset.map(tokenize, batched=False)
dataset.set_format(type="torch")

def collator(data):
    return {key: [d[key] for d in data] for key in data[0]}

# --- 4. åˆå§‹åŒ– PPO Trainer ---
ppo_trainer = PPOTrainer(
    config=config,
    model=model,
    ref_model=ref_model,
    tokenizer=tokenizer,
    dataset=dataset,
    data_collator=collator,
)

# --- 5. è®­ç»ƒå¾ªç¯ ---
generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.pad_token_id,
    "max_new_tokens": 100, # æ‘˜è¦é•¿åº¦æ§åˆ¶
}



print("ğŸš€ å¼€å§‹ PPO è®­ç»ƒ...")
for epoch, batch in enumerate(tqdm(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    # A. Policy æ¨¡å‹ç”Ÿæˆæ‘˜è¦
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r, skip_special_tokens=True) for r in response_tensors]

    # B. æ„é€  RM çš„è¾“å…¥å¹¶æ‰“åˆ†
    # æ³¨æ„ï¼šRM è¯„ä¼°æ—¶é€šå¸¸éœ€è¦å¯¹é½è®­ç»ƒæ—¶çš„æ‹¼æ¥æ ¼å¼
    texts = [q + r + tokenizer.eos_token for q, r in zip(batch["query"], batch["response"])]
    
    # åˆ‡æ¢åˆ° RM éœ€è¦çš„å³å¡«å……æ¨¡å¼è¿›è¡Œæ¨ç†ï¼ˆä¸´æ—¶æ“ä½œï¼‰
    tokenizer.padding_side = "right"
    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True).to(device)
    with torch.no_grad():
        rewards = reward_model(**inputs).logits.squeeze(-1)
    
    # å¥–åŠ±å½’ä¸€åŒ–å¤„ç†ï¼ˆå¯é€‰ï¼šå‡å»å‡å€¼ï¼Œè®©å¥–åŠ±æœ‰æ­£æœ‰è´Ÿï¼‰
    rewards = [torch.tensor(r) for r in rewards]
    tokenizer.padding_side = "left" # åˆ‡å›ç”Ÿæˆæ¨¡å¼

    # C. æ‰§è¡Œ PPO æ­¥è¿›
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    
    # D. æ‰“å°å…³é”®æŒ‡æ ‡
    if epoch % 10 == 0:
        ppo_trainer.log_stats(stats, batch, rewards)
        print(f"Epoch {epoch} | Mean Reward: {stats['ppo/returns/mean']:.4f} | Mean KL: {stats['objective/kl']:.4f}")

# --- 6. ä¿å­˜æ¨¡å‹ ---
ppo_trainer.save_pretrained("./ppo-tldr-final")
print("âœ… PPO è®­ç»ƒå®Œæˆå¹¶ä¿å­˜ï¼")